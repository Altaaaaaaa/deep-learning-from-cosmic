{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5   \n",
    "### 순전파와 역전파   \n",
    "문제를 계산그래프로 풀 때    \n",
    "<li>왼쪽에서 오른쪽 : 순전파</li>   \n",
    "<li>오른쪽에서 왼쪽 : 역전파</li>   \n",
    "계산그래프의 특징으로는 <b>국소적 계산</b>이 있다.   \n",
    "각각의 계산은 앞의 계산과는 상관없이 본인의 계산만 하면 된다. 따라서 역전파를 했을 때 미분을 효율적으로 계산할 수 있는 장점이 있다.   \n",
    "\n",
    "### 연쇄법칙   \n",
    "<i>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</i>   \n",
    "$$ {\\delta z \\over \\delta x} = {\\delta z \\over \\delta t}{\\delta t \\over \\delta x} $$   \n",
    "이를 통해 계산 그래프의 역전파는 국소적 미분값들의 곱을 통해 최종적으로 미분값을 나타낼 수 있다.   \n",
    "\n",
    "### 덧셈과 곱셈 노드의 역전파   \n",
    "<b> 덧셈노드</b>\n",
    "$ {\\delta L \\over \\delta x }  $   \n",
    "<br>\n",
    "<b> 곱셈노드</b>\n",
    "$ {\\delta L \\over \\delta x }  {\\delta x \\over \\delta y} $\n",
    "\n",
    "덧셈노드의 경우 1을 곱하는 것이기 때문에 입력 신호 값이 필요 없지만, 곱셈노드의 경우 순방향 입력 신호 값이 필요하기 때문에 순전파의 입력 신호를 저장해둔다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): #역전파\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer= MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout  * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "2.2 110.00000000000001 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple, dapple_num, dorange, dorange_num, dtax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산 그래프에서 계층의 구현은 쉽게 구현할 수 있으며, 이를 사용해 복잡한 미분도 효율적으로 계산할 수 있다.   \n",
    "\n",
    "ReLU 계층 구현하기   \n",
    "$$\n",
    "y = \\begin{cases}x&(x>0)\\\\0&(x\\le 0)\\end{cases}\\\\\n",
    "{\\delta y \\over \\delta x} = \\begin{cases}1&(x>0)\\\\0&(x\\le 0)\\end{cases}\n",
    "$$   \n",
    "\n",
    "Sigmoid 계층 구현하기   \n",
    "$$\n",
    "y = {1 \\over 1 + exp(-x)}\n",
    "$$   \n",
    "이 때 위 식을 미분하는 것은 복잡하기 때문에 아래 그림처럼 나누어서 미분을 진행한다.     \n",
    "![sigmoid_forward_graph](./images/sigmoid_foward_graph.png)   \n",
    "### 역전파 순서   \n",
    "1. '/'   \n",
    "${\\delta L \\over \\delta y} = -{1 \\over x^2} = -y^2$   \n",
    "2. '+' 그대로 통과   \n",
    "$-{\\delta L \\over \\delta y}{y^2}$   \n",
    "3. 'exp' 지수함수의 미분은 $\\log_eaa^x$\n",
    "$-{\\delta L \\over \\delta y}{y^2}exp(-x)$   \n",
    "4. '$\\times$' $y = -x \\to {\\delta y \\over \\delta x} = -1$   \n",
    "${\\delta L \\over \\delta y}{y^2}exp(-x)$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "{\\delta L \\over \\delta y}{y^2}exp(-x)\\\\\n",
    "{1\\over y}-1 = exp(-x)\\\\\n",
    "={\\delta L \\over \\delta y}{y^2}({1\\over y}-1)\\\\\n",
    "={\\delta L \\over \\delta y}{y}(1-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1+np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rnad(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">순전파 때 수행하는 행렬의 내적은 기하학에서 어파인 변환(Affine transformation)이라고 합니다.   위 책에서는 어파인 변환을 수행하는 처리를 'Affine 계층'이라는 이름으로 구현합니다.   \n",
    "\n",
    "$\n",
    "{\\delta L \\over \\delta X} = {\\delta L \\over \\delta Y}\\cdot W^T\\\\\n",
    "{\\delta L \\over \\delta W} = X^T \\cdot {\\delta L \\over \\delta Y}\n",
    "$   \n",
    "\n",
    "X = (N, x) W = (x, w) B = (N, w) Y = (N, w)   \n",
    "X에 대한 변화량을 구하려면 $Y\\cdot W^T$      \n",
    "(N, w) (w, x) = (N, x)   \n",
    "W에 대한 변화량을 구하려면 $X^T \\cdot Y$   \n",
    "(x, N) (N, w) = (x, w)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 dx의 변화량을 구하는 것일까?   \n",
    "x는 고정된 값이고 w가 수정되는 값이므로 dw를 구하는 것이 맞지 않나?   \n",
    "근데 여기서 생각해봐야 하는 것이 여기에서 x는 데이터 자체 일수도 있지만 중간 계층의 결과물일 수도 있기 때문에 dx와 dw를 둘 다 구하는 것이 맞는 것 같다.   \n",
    "\n",
    "###### Softmax layer   \n",
    "$(a_1, a_2, a_3)$   normalize   $(y_1, y_2, y_3)$   \n",
    "###### Cross Entropy Error   \n",
    "$(y_1, y_2, y_3)$ & $(t_1, t_2, t_3)$ output $L$   \n",
    "\n",
    "여기에서 softmax 계층의 역전파는 $(y_1-t_1, y_2-t_2, y_3-t_3)$를 나타낸다. 왜?   \n",
    "\n",
    "![cross_entropy_error_backward](./images/cross_entropy_error_backward.png)\n",
    "\n",
    "<b>계산 그래프 역전파의 규칙</b>   \n",
    "1. 역전파의 초깃값은 1이다.   \n",
    "2. $\\times$ 노드의 역전파는 순전파 시의 입력값을 서로 바꿔 상류의 미분에 곱하고 하류로 흘린다.   \n",
    "3. '+'노드에서는 상류에서 전해지는 미분을 그대로 흘린다.   \n",
    "4. '/' 노드의 역전파는 상류에서 흘러온 값에 순전파 때의 출력을 제곱한 후 마이너스를 붙인 값을 곱해 하류로 전달한다.   \n",
    "5. log 노드의 역전파는 $y = \\log x \\to {\\delta y \\over \\delta x} = {1 \\over x}$를 따른다.   \n",
    "6. 순전파 때 갈라진 것은 역전파 때 값을 합치면 된다.   \n",
    "7. 순전파 때 합쳐진 것은 역전파 때 값을 나누면 된다.   \n",
    "\n",
    "위 규칙들을 이용하여 cross-entropy-error 에서 softmax 까지의 역전파가 $y_k - t_k$인 것을 알 수 있다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 미니배치   \n",
    "    훈련 데이터 중 일부를 무작위로 가져오고 그 미니배치의 손실함수 값을 줄이는 것이 목표이다.   \n",
    "2. 기울기 산출   \n",
    "    미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 작게 하는 방향을 제시한다.   \n",
    "3. 매개변수 갱신   \n",
    "    가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.   \n",
    "4. 반복   \n",
    "    1~3단계를 반복한다.   \n",
    "\n",
    "오차역전파법은 매개변수가 많아도 효율적으로 계산할 수 있고 계층이 추가적으로 필요하다면 레고 블럭처럼 조립할 수 있다는 장점이 있다. 하지만 오차역전파법은 구현하기 복잡해서 종종 실수가 있을 수 있기 때문에 수치미분법을 통해 기울기 검증을 해야한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1) # output one-hot-lable\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1) # one-hot-lable\n",
    "\n",
    "        accuray = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuray\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        #아래의 numerical_gradient는 common.gradient.py안에 있는 numerical_gradient\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11361666666666667 0.109\n",
      "0.90375 0.9101\n",
      "0.9279666666666667 0.9288\n",
      "0.9404166666666667 0.9392\n",
      "0.9460333333333333 0.9431\n",
      "0.9516833333333333 0.9479\n",
      "0.95705 0.9548\n",
      "0.9600333333333333 0.9569\n",
      "0.9636833333333333 0.9586\n",
      "0.9667 0.9627\n",
      "0.9689166666666666 0.9634\n",
      "0.97035 0.9649\n",
      "0.9732 0.9664\n",
      "0.9735333333333334 0.9671\n",
      "0.9762 0.9682\n",
      "0.9769666666666666 0.9697\n",
      "0.97815 0.9697\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=\"True\")\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea6d7042cb93db4bc8be5a69b26cbfd7e48eef1f556879fc2bfcfe6997ec0c28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
