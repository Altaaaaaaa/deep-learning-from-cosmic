{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론으로 복잡한 함수를 표현할 수 있다. 하지만 가중치는 사람이 수동적으로 설정하고한다. 이러한 문제를 해결하기 위해 신경망을 사용한다. 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력을 주는 것이다.\n",
    "image.png\n",
    "위의 그림처럼 입력층, 은닉층,출력층으로 구성되어있다.\n",
    "활성화 함수h(x)를 표현하면 y=h(b+w1*x1+w2*x2)이 되고\n",
    "x가 0보다크면 1 작으면 0을 반환한다.\n",
    "활성화 함수는 입력신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다.\n",
    "a=b+w1*x1+w2*x2\n",
    "y=h(a)\n",
    "image.png\n",
    "뉴런과 노드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt#그래프를 그리는 패키지\n",
    "def step_function(x):#x가 0보다 크면 1반환 0보다 작으면 0반환\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수에서 임계값을 경계로 출력이 바뀌는데 이런 함수를 계단 함수라고한다.하지만 이외에도 다른 함수로도 신경망의 세계를 나아갈 수 있음\n",
    "시그모이드 함수->image.png\n",
    "이러한 함수를 이용하여 신호를 변환하고 변환한 신호를 뉴런에 전달한다. 퍼셉트론과 신경망의 차이는 활성화함수이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):#계단함수\n",
    "    y=x>0\n",
    "    return y.astype(np.int)# y의 타입을 int로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([-1.0,1.0,2.0])\n",
    "print(x)\n",
    "y=x>0 #bool형인 y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.astype(np.int)#bool배열인 y를 int로 바꿔줌\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    return np.array(x>0,dtype=np.int)#np안에 배열에서 x가 0보다 큰 수는 int형으로 반환\n",
    "x=np.arange(-5.0,5.0,0.1)#x에 -5부터 5까지 0.1간격으로된 배열을 넣는다.\n",
    "y=step_function(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)#y축\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))#sigmoid함수\n",
    "\n",
    "x=np.array(([-1.0,1.0,2.0]))#x의 배열\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.array([1.0,2.0,3.0])\n",
    "1.0+t\n",
    "\n",
    "1.0/t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(-5.0,5.0,0.1)\n",
    "y=sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수와 계단 함수의 공통되는 성질은0또는 1의 출력값을 준다는 것입니다. 또, 입력이 작을 때는 출력이 0에 가깝고 입력이 커지면 1에 가까워진다, 둘다 비선형 함수이다.\n",
    "다음으로 차이는 매끄러움입니다. 시그모이드는 부드러운 곡선이고 입력에 따라출력이 연속적으로 변화\n",
    "계단 함수는 0에서 1로 출력이 갑자기 바뀜\n",
    "계단함수는 0과 1만 출력하지만 시그모이드는 실수를 돌려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에서는 활성화 함수로 비선형 함수를 사용해야한다. 왜냐하면 선형 함수는 신경망의 층을 깊게하는 의미가 없어지기 때문이다. 아무리 c*c*c*x라도 a=c*c*c인 수를 x와 곱하는 단층 퍼셉트론이 있다면 끝이다. 즉 은닉층이없는 네트워크로 표현할 수 있다.그렇기 때문에 층을 쌓는 혜택을 받기위해서는 활성화 함수로 비선형 함수를 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)#relu함수 np배열에 있는 값중 가장 큰 값을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu는 입력이 0을 넘으면 그 입력 그대로 0이하면 0출력하는 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A=np.array(1,2,3,4)\n",
    "print(A)\n",
    "print(np.ndim(A))#A의 차원 수\n",
    "print(A.shape)#A의 행렬 형상\n",
    "print(A.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=np.array([[1,2],[3,4],[5,6]])\n",
    "print(B)\n",
    "np.ndim(B)#B행렬의 차원 2\n",
    "B.shape#B행렬의 형상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 배열에서 행은 가로 열은 세로이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,2],[3,4]])\n",
    "A.shape\n",
    "B=np.array([[5,6],[7,8]])\n",
    "B.shape\n",
    "np.dot(A,B)#np에서 A,B배열을 내적한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬의 내적은 왼쪽 행렬의 행과 오른 쪽 행렬의 열을 원소별로 곱하고 그 값들을 더해 계산하는 것이다.\n",
    "여기서 np.dot(A,B)!=np.dot(B,A)이다\n",
    "내적은 행렬의 형상의 주의해야한다. A행렬의 열과 B행렬의 행의 수가 같아야한다. A(2,3) B(3,2)->C(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,2,3,],[4,5,6]])\n",
    "A.shape#(2,3)\n",
    "B=np.array([[1,2],[3,4],[5,6]])\n",
    "B.shape#3,2\n",
    "np.dot(A,B)#np에서 A,B배열을 내적한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.array([[1,2],[3,4]])\n",
    "C.shape(2,2)\n",
    "A.shape(2,3)\n",
    "np.dot(A,C)#행과 열의 수가 같지 않기 때문에 A,C배열을 내적하지 못한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,2],[3,4],[5,6]])\n",
    "A.shape#(3,2)\n",
    "B=np.array([7,8])#(2,1)\n",
    "B.shape\n",
    "np.dot(A,B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([1,2])#입력값\n",
    "X.shape#(1,2)\n",
    "W=np.array([[1,3,5],[2,4,6]])#가중치\n",
    "print(W)\n",
    "W.shape#(2,3)\n",
    "Y=np.dot(X,W)#X와 W의 행렬의 차원이 일치하기 때문에 내적가능\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W오른쪽 위(n)->n층의 가중치\n",
    "오른쪽 밑1->다음 층의 1번째 뉴런 2->앞층의 2번째뉴런\n",
    "A(1)=XW(1)+B(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([1.0,0.5])\n",
    "W1=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "B1=np.array([0.1,0.2,0.3])\n",
    "\n",
    "print(W1.shape)\n",
    "print(X.shape)\n",
    "print(B1.shape)\n",
    "\n",
    "A1=np.dot(X,W1)+B1 #X와W1이 내적된 후 B1과 더해져 은닉층에 값이 출력됨\n",
    "Z1= sigmoid(A1) #h()활성화 함수인 시그모이드를 거쳐 z1이 출력됨\n",
    "\n",
    "print(A1)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화 함수를 거치면서 활성화 된다면\n",
    "z값을 받게 되면서 층을 거치면서 새로운 y값을 얻을 수 있게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "B2=np.array([0.1,0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2=np.dot(Z1,W2)+B2\n",
    "Z2=sigmoid(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_function(x):#항등 함수\n",
    "    return X\n",
    "W3=np.array([[0.1,0.3],[0.2,0.4]])\n",
    "B3=np.array([0.1,0.2])\n",
    "\n",
    "A3=np.dot(Z2,W3)+B3\n",
    "Y=identity_function(A3) #y를 구현하기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_function(x):#항등 함수\n",
    "    return X\n",
    "\n",
    "def init_network():# 네트워크 초기화 함수\n",
    "    network={}#network 배열에 딕셔너리를 사용하여 key와 value에 맞게 찾을 수 있게 만든다.\n",
    "    network['W1']=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "    network['b1']=np.array([0.1,0.2,0.3])\n",
    "    network['W2']=np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "    network['b2']=np.array([0.1,0.2])\n",
    "    network['W3']=np.array([[0.1,0.3],[0.2,0.4]])\n",
    "    network['b3']=np.array([0.1,0.2])\n",
    "    \n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1,W2,W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1,b2,b3 = network['b1'],network['b2'],network['b3']\n",
    "    \n",
    "    a1=np.dot(x,W1)+b1\n",
    "    z1=sigmoid(a1)\n",
    "    a2=np.dot(z1,W2)+b2\n",
    "    z2=sigmoid(a2)\n",
    "    a3=np.dot(z2,W3)+b3\n",
    "    y=identity_function(a3)    \n",
    "    \n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x=np.array([1.0,0.5])\n",
    "y=forward(network,x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크를 초기화하고 난 뒤의 network변수와 x입력값들을 forward함수에 넣고 \n",
    "출력값을 도출해낸다. 이는 y이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 분류와 회귀 모두에 이용된다. 둘중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라진다. 일반적으로 회귀에는 항등 함수를, 분류에는 소프트맥스 함수를 사용한다.\n",
    "분류는 데이터가 어느 클래스에 속하느냐는 문제임\n",
    "회귀는 입력데이터에서 (연속적인) 수치를 예측하는 문제임\n",
    "\n",
    "분류에서 사용하는 소프트맥스 함수는\n",
    "y=exp(ak)/모든exp(a1~n)\n",
    "분자는 입력 신호ak의 지수 함수, 분모는 모든 입력 신호의 지수 함수의 합으로 구성된다.\n",
    "소프트맥스 함수는 출력이 모든 입력신호로부터 화살표를 받는다. 즉 모든 입력신호에 영향을 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([0.3,2.9,4.0])\n",
    "\n",
    "exp_a=np.exp(a)#a배열에 있는 수\n",
    "print(exp_a)\n",
    "\n",
    "sum_exp_a=np.sum(exp_a)#exp_a에 있는 모든 수를 다 더한 수\n",
    "print(sum_exp_a)\n",
    "\n",
    "y=exp_a/sum_exp_a#softmax함수\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a=np.exp(a)\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax함수는 컴퓨터로 계산하면 오버플로 문제발생 가능성이 있음.\n",
    "지수함수를 사용하기 때문에 기하급수적으로 증가\n",
    "지수함수를 계산할 때 어떤 정수를 더하고 빼도 결과는 바뀌지 않는다. \n",
    "오버플로를 막을 목적으로는 입력신호중 최댓값을 이용하는것이 일반적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1010,1000,990])#입력신호\n",
    "np.exp(a)/np.sum(np.exp(a))#softmax함수\n",
    "c=np.max(a)#a입력값중 최대값\n",
    "a-c#a배열들을 c로 다 빼줌\n",
    "np.exp(a-c)/np.sum(np.exp(a-c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([0.3,2.9,4.0])\n",
    "y=softmax(a)\n",
    "print(y)\n",
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스 함수의 특징\n",
    "소프트맥스 함수의 출력은 0~1사이의 실수이다. 또 소프트맥스 함수 출력의 총합은 1이다.\n",
    "이 성질덕분에 소프트맥스 함수의 출력을 확률로 해석할 수 있음.->통계적으로 대응가능\n",
    "*주의점 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않음(y=exp(x)단조 증가함수이기 때문)\n",
    "\n",
    "신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식한다.\n",
    "소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치 달라지지않음\n",
    "결과적으로 신경망으로 분류할 때 소프트맥스 함수는 생략해도 됨.\n",
    "현업에서도 지수함수계산에 드는 자원낭비를 줄이고자 출력층의 소프트맥스 함수는 생략함\n",
    "\n",
    "기계학습의 문제풀이는 학습과 추론의 두단계를 거쳐 이뤄진다. 학습단계에서 모델을 학습하고, 추론 단계에서 앞서 학습한 모델로 미지의 데이터에 대해서 추론(분류)을 수행한다. 추론단계에서는 소프트맥스함수를 생략하고 학습시킬 때는 출력층에서 소프트맥스 함수를 사용한다.\n",
    "\n",
    "->학습할 때는 확률에 따라 더 높은 확률인것을 선택하도록 만들어야하기 때문에 소프트맥스 함수를 사용함으로 써 더 높은 확률을 가진 출력값을 찾고 학습할 것같음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 순전파->추론과정\n",
    "MNIST 데이터셋- 손글씨 숫자 이미지 집합이다.\n",
    "일반적으로 훈련이미지를 사용하여 모델을 학습하고 학습한 모델로 시험 이미지들을 얼마나 정확하게 분류하는지 평가함.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)#부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train),(x_test,t_test)= load_mnist(flatten=True,normalize=False)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)#데이터들의 형상 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist #mnist..py의 load_mnist함수를 임포트함\n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)  # 5\n",
    "\n",
    "print(img.shape)  # (784,)\n",
    "img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "print(img.shape)  # (28, 28)\n",
    "\n",
    "img_show(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_mnist함수는 MNIST데이터를 \"(훈련 이미지, 훈련 레이블),(시험 이미지, 시험 레이블\")형식으로 반환한다. 인수로는 normalize,flatten,one_hot_label세가지로 설정한다. 세 인수 모두 bool값이다.\n",
    "첫 번째 인수인 normalize는 입력 이미지의 픽셀 값을 0.0~1.0사이의 값으로 정규화할지를 정합니다. False로 설정하면 입력 이미지의 픽셀은 원래 값 그대로 0~255사이의 값유지한다.\n",
    "두 번째 인수인 flatten은 입력 이미지를 평탄하게, 즉 1차원 배열로 만들지를 정합니다. False로 설정하면 입력 이미지를 1*28*28의 3차원 배열로 True로 설정하면 784개의 원소로 이뤄진 1차원 배열로 저장합니다. \n",
    "세 번째 인수인 one_hot_label은 레이블을 원-핫 인코딩 형태로 저장할지 정한다.\n",
    "true이면 원 핫 인코딩하고 0이면 '7'이나 '2'와 같은 숫자형태 레이블 저장\n",
    "flatten=True로 설정해 읽어 들인 이미지는 1차원 넘파이 배열로 저장되어있음\n",
    "그래서 28*28크기로 다시 변형해야함. reshape메서드는 원하는 형상을 인수로 지정하면 넘파이 배열의 형상을 바꿀 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) =\\\n",
    "        load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():#pickle파일인 sample_weight.pkl에 저장된 '학습된 가중치 매개변수'를 읽음#이 파일에는 가중치와 편향 매개변수가 딕셔너리 변수로 저장됨\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:#파일 \"sample_weight.pkl을 열고 변수들을 network딕셔너리에 넣음\"\n",
    "        network = pickle.load(f)\n",
    "        \n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle이라는 모듈이 필요함\n",
    "pickle은 텍스트 상태의 데이터가아닌 파이썬 객체 자체를 파일로 저장하는 것\n",
    "데이터를 특정 범위로 변환하는 처리 정규화\n",
    "신경망의 입력 데이터에 특정 변환을 가하는 것 전처리\n",
    "여기서는 입력 이미지 데이터에 대한 전처리 작업으로 정규화를 수행한 셈이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t =get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.\n",
    "    if p == t[i]:\n",
    "        accuracy_cnt +=1\n",
    "        \n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = get_data()\n",
    "network = init_network()\n",
    "W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "x.shape\n",
    "x[0].shape\n",
    "W1.shape\n",
    "W2.shape\n",
    "W3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나로 묶은 입력 데이터를 배치라 한다. \n",
    "배치 처리는 컴퓨터로 계산할 때 좋다. \n",
    "1) 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도로 최적화되어 있다. \n",
    "2) 배치 처리를 함으로써 버스에 주는 부하를 줄일 수 있다. (느린 I/O를 통해 데이터를 읽는 횟수가 줄어, 빠른 CPU나 GPU로 순수 계산을 수행하는 비율이 높아진다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100 # 배치 크기\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis = 1)\n",
    "    accuracy_cnt+=np.sum(p==t[i:i+batch_size])\n",
    "    \n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[0.1,0.8,0.1],[0.3,0.1,0.6],[0.2,0.5,0.3],[0.8,0.1,0.1]])\n",
    "y=np.argmax(x,axis=1)#arg max->최댓값의 인덱스를 가져옴=axis=1 100*10 배열중 1번째 차원을 구성하는 각 원소에서 최댓값의 인덱스를 찾도록 한 것이다.\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46e0f8da782ee4e5836de201b49d269b788967913c4102a2cdd310d10725f3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
