{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 흭득하는 것을 뜻한다.\n",
    "신경망이 학습할 수 있도록 해주는 지표인 손실 함수가 있다.->결과값을 가장 작게 만드는 가중치 매개변수를 찾아야함.(함수의 기울기를 활용하는 경사법)\n",
    "\n",
    "신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. \n",
    "데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻이다.(수작업으로 가중치를 매길 필요x)\n",
    "기계학습은 데이터에서 답을 찾고 패턴을 발견하고 이야기를 만든다. \n",
    "사람은 경험과 직관을 단서로 시행착오로 일을 진행하는데 기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.(사람의 개입을 배제할 수 있게함.)\n",
    "\n",
    "기계학습 방법: 1.이미지에서 특징을 추출하고 그 특징의 패턴을 기계학습 기술로 학습함.\n",
    "특징->입력데이터에서 본질적인 데이터(중요 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리킴.\n",
    "이미지의 특징을 벡터로 변환하고 변환된 벡터를 지도 학습 방식의 SVM,KNN등으로 학습한다.('사람'이 설계함-적합한 특징을 쓰지 않으면 결과 안좋을 수 있음)\n",
    "기계학습에서 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당함.\n",
    "즉 특징은 사람이 설계하지만 신경망은 이미지에 포함된 중요한 특징까지 '기계'가 스스로 학습한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습에서 데이터 취급시 주의할 점 (범용 능력을 평가하기 위해)\n",
    "1.훈련데이터로만 사용하여 학습하면서 최적의 매개변수를 찾음\n",
    "2.시험 데이터를 사용하여 훈련한 모델의 실력을 평가함.\n",
    "\n",
    "범용능력이란 아직 보지 못한 데이터로도 문제를(포함X)올바르게 풀 능력\n",
    "기계학습의 최종 목표임.\n",
    "한 데이터셋에서만 지나치게최적화된 상태 오버피팅->포함되지 않는 것도 찾을 수 있어야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 손실함수\n",
    "신경망 학습에서 현재의 상태를 지표로 표현한다-> 가중치 매개변수의 값을 탐색함.\n",
    "1. 평균 제곱 오차 손실함수\n",
    "모든 신경망의출력에서 정답레이블을 뺀후 제곱해서 더한 수의 2를 나눈것을 평균 제곱 오차라고한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "def mean_squared_error(y,t): #평균 제곱 오차함수\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] #소프트 맥스함수의 출력모든 확률들을 다 더하면 1이된다.\n",
    "t=[0,0,1,0,0,0,0,0,0,0] #2번째를 제외한 나머지는 0이므로 원 핫 인코딩임을 알수있음\n",
    "\n",
    "mean_squared_error(np.array(y),np.array(t))#입력과 출력이 모두 '2'이기 오차가 적음을 알 수 있다.\n",
    "\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]#'7'일 확률이 가장 높다고 추정하면\n",
    "\n",
    "mean_squared_error(np.array(y),np.array(t))# 입력은 '7'이지만 출력은 '2'이기 때문에 오차가 큰 것으로 틀렸다는 것을 알 수 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2 교차 엔트로피 오차 손실함수\n",
    "t*logy를 모두 더한 후 전체 -한 함수이다.\n",
    "t가 정답만 1이고 나머지는 0이기때문에 정답일 때의 추정의 자연로그를 계산하는 식이 된다.(t가 1일때만 y가 나옴)\n",
    "x가 1일 때 y는 0이 되고 x가0에가까워질 수록 y의 값은점점작아진다.\n",
    "정답이면 작아짐 즉 오차일 확률이 적어지는 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7 #log()함수에 0을 입력하면 마이너스 무한대를 뜻하기 때문에 0이 되지 않도록 만든 것임.\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]#'2'가 정답일 때\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "cross_entropy_error(np.array(y),np.array(t))\n",
    "\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0] #'7'가 정답일 때\n",
    "cross_entropy_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3미니배치 학습\n",
    "기계학습은 훈련데이터에 대한 손실함수의 값을 구하고 그 값을 최대한 줄여주는 매개변수를 찾는다. ->모든 훈련데이터의 손실함수값을 구해서 합을 지표로 삼아야함.\n",
    "데이터가 N개라면 tnk는 n번째 데이터의 k번째 값을 의미한다.(ynk는 신경망의 출력,tnk는 정답레이블) \n",
    "E=-N/SUM(N까지)SUM(K까지)tnk*log(ynk)\n",
    "N개의 데이터로 확장한 후 N으로 나누어 정규화한 것임.->평균손실함수를 구한것\n",
    "이러한 방식으로 평균을 구하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다.\n",
    "하지만 빅데이터를 손실함수로 계산하는 것은 힘들다 이런 경우 데이터 일부를 전체의 근사치로 이용할 수 있다. 일부만 골라학습\n",
    "이를 미니배치라고한다.\n",
    "\n",
    "60000장의 훈련 데이터중에서 100장을 무작위로 뽑아 100장만 학습하는것을 미니배치학습이라고한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12734, 41841, 38601, 45827, 27929, 20335, 44511, 46965, 29443,\n",
       "       35332])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist #훈련데이터와 시험데이터를 읽는다.\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=\\\n",
    "    load_mnist(normalize=True,one_hot_label=True)\n",
    "    #훈련데이터는 60000개인데\n",
    "print(x_train.shape)#입력데이터784개\n",
    "print(t_train.shape)#정답레이블 10\n",
    "\n",
    "train_size=x_train.shape[0] #60000\n",
    "batch_size=10 #10\n",
    "batch_mask=np.random.choice(train_size,batch_size)# train_size중에서batch_size만큼 랜덤으로 뽑겠다.\n",
    "x_batch=x_train[batch_mask]\n",
    "t_batch=t_train[batch_mask]\n",
    "np.random.choice(60000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩으로 주어졌을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimju\\AppData\\Local\\Temp\\ipykernel_1736\\269628465.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(t*np.log(y))/batch_size# batch_size가 크기임\n",
      "C:\\Users\\kimju\\AppData\\Local\\Temp\\ipykernel_1736\\269628465.py:7: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.sum(t*np.log(y))/batch_size# batch_size가 크기임\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:# y가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 reshape 함수로 데이터의 형상을 바꾼다.\n",
    "        t=t.reshape(1,t.size)\n",
    "        y=y.reshape(1,y.size)\n",
    "\n",
    "    batch_size=y.shape[0]\n",
    "    return -np.sum(t*np.log(y))/batch_size# batch_size가 크기임\n",
    "\n",
    "\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]#'2'가 정답일 때\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "cross_entropy_error(np.array(y),np.array(t))\n",
    "# 형상을 바꾼 후 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        t=t.reshape(1,t.size)\n",
    "        y=y.reshape(1,y.size)\n",
    "    batch_size=y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t]))/batch_size#t가 0이면 교차 엔트로피 오차도 0이므로 무시\n",
    "#정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있음.\n",
    "#np.arange(batch_size)는 0부터 batch_size-1까지 배열을 생성하는 것이다. \n",
    "#즉 y[배열,t배열]=>[y[0,2],y[1,7]....]인 넘파이 배열이 생성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수를 사용하는 이유\n",
    "1. 높은 정확도를 끌어내는 매개변수 찾기위해->그렇다면 정확도라는 지표놔두고 손실 함수이 값이라는 우회적인 방법을 택하는 이유?->미분\n",
    "신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 작게하는 매개변수 값을 찾는다. 이때 매개변수의 미분(기울기)를 계산하고 그 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.\n",
    "가중치 매개변수의 손실 함수 미분이란 '기중치 매개변수의 값을 아주 조금 변화 시킬 때, 손실함수의 변화'를 의미함\n",
    "미분이 음수라면 가중치 매개변수를 양의 변화를 만들어 손실 함수의 값을 줄이고 반대로 미분 값이 양수면 손실 함수의 값을 높일 수 있다. 만약 미분값이 0이면 갱신은 멈춘다.\n",
    "\n",
    "정확도를 지표로 삼아서는 안되는 이유는\n",
    "미분 값이 대부분의 장소에서 0이 되기 때문에 매개변수 갱신x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 수치 미분\n",
    "미분은 x의 '작은변화'가 함수f(X)를 얼마나 변화시키느냐를 의미함.\n",
    "이때 시간의 작은 변화, 즉 시간을 뜻하는h를 한없이 0에 가깝하게 한다는 의미로 lim를 쓴다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=10e-50\n",
    "    return (f(x-h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드에서의 문제\n",
    "1. 반올림오차\n",
    "0.000000...1형태이지만 반올림 오차만듬 np.float32(1e-50)=0.0임 올바른 표현x\n",
    "h->10^-4로 고친다.\n",
    "2. 함수f의 차분(x+h와 x사의 함수f의 차분)\n",
    "위 코드는 x와x+h사이의 기울기에 해당한다. 그래서 진정한 미분x(이 차이는 무한히 0으로 좁히지 못해 생기는 한계)\n",
    "이를 해결하기 위해 x-h일 때의 함수f의 차분을 계산하는 방법을 씀\n",
    "이 차분은 x를 중심으로 그전후 차분을 계산한다는 의미에서 중심 차분 혹은 중앙차본이라함\n",
    "한편 x+h와 x의 차분은 전방차분이라함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4\n",
    "    return (f(x+h)-f(x-h)/2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 차분으로 미분하는 것을 수치미분이라한다. 수식을 전개해 미분하는것은 해석적으로 미분한다고함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y=0.01x^2+0.1x가 있다고하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kimju\\OneDrive\\바탕 화면\\Junho\\deep-learning-from-cosmic\\junho\\4주차\\4주차_김준호.ipynb 셀 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000018?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.01\u001b[39m\u001b[39m*\u001b[39mx\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39m\u001b[39m0.1\u001b[39m\u001b[39m*\u001b[39mx\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000018?line=3'>4</a>\u001b[0m x\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0.0\u001b[39m,\u001b[39m20.0\u001b[39m,\u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000018?line=4'>5</a>\u001b[0m y\u001b[39m=\u001b[39mfunction(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000018?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000018?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mf(x)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'function' is not defined"
     ]
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2+0.1*x\n",
    "\n",
    "x= np.arange(0.0,20.0,0.1)\n",
    "y=function(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numerical_diff(function_1,5))\n",
    "print(numerical_diff(function_1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 계산한 미분 값이 x에 대한 f(x)의 변화량이다.(함수의 기울기)\n",
    "f(x)를 미분한 식에 5와 10을 대입하면 매우 작은 오차임을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x0,x1)=x0^2+x1^2\n",
    "인수x는 넘파이 배열이라고 가정한다. 이를 그래프로 그리면 3차원으로 그려진다.\n",
    "이를 미분하려고할 때 두 변수중 한개에 초점을 맞춰야하는데 이처럼 여럿인 함수에 대한 미분을 편미분이라고한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "   return x[0]**2 + x[1]**2#return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0+4.0**2.0 #x0만 미분할 수 있도록\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0+x1*x1\n",
    "\n",
    "print(numerical_diff(function_tmp1,3.0))\n",
    "print(numerical_diff(function_tmp2,4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 두 함수는 변수 하나를 고정하여 편미분한 예이다.\n",
    "이처럼 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 기울기\n",
    "만약 모든 변수의 편미분을 동시에 계산하고 싶다면 양쪽의 편미분을 묶어서 계산해야한다. 모든 변수의 편미분을 벡터로 정리한 것을 기울기라한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x)#x와 형상이같고 그 원소가 모두 0인 배열을 만듦\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val=x[idx]\n",
    "        #f(x+h계산)\n",
    "        x[idx]=tmp_val+h\n",
    "        fxh1=f(x)\n",
    "        \n",
    "        #f(x-h)계산\n",
    "        x[idx]=tmp_val-h\n",
    "        fxh2=f(x)\n",
    "        \n",
    "        grad[idx]=(fxh1-fxh2)/(2*h)\n",
    "        x[idx]=tmp_val#값복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numerical_gradient함수의 인수인 f는 함수이고 x는 넘파이 배열이므로 넘파이 배열 x의 각 원소에 대해서 수치미분을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "   return x[0]**2 + x[1]**2#return np.sum(x**2)\n",
    "\n",
    "numerical_gradient(function_2,np.array([3.0,4.0]))\n",
    "numerical_gradient(function_2,np.array([0.0,2.0]))\n",
    "numerical_gradient(function_2,np.array([3.0,0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기는 방향을 가진 벡터로 그려진다. '가장 낮은 장소'를 가리킨다. \n",
    "가장 낮은 곳에서 멀어질 수록 화살표의 크기가 커진다.\n",
    "즉 기울기가 가리키는쪽은 각 장소에서의 함수의 출력값이 가장 크게 줄어드는 방향입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습 학습단계에서 최적의 매개변수를 찾아야한다 신경망역시 가중치와 편향을 찾아햐나는데 여기서 최적이란 손실 함수가 최솟값이 될 때의 매개변수값이다.\n",
    "하지만 일반적인 손실함수는 복잡해 최솟값을 찾기어려운데 이 때 기울기를 이용해 함수의 최솟값을 찾는다.\n",
    "주의할 점은 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기이다. 하지만 기울기가 가리키는 곳이 함수의 최솟값인지 그쪽이 나은 방향인지 보장할 수 없다.\n",
    "꼭 그 방향이 최솟값은 아니지만 그 방향으로 가야 함수의 값이 줄어든다. 그래서 최솟값이 되는 장소를 찾는 문제는 기울기 정보로 단서로 나아갈 방향정해야한다.\n",
    "\n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그 후 기울기를 구하고 또 기울어진 방향으로 나아가기를 반복한다. 이로인해 함수의 값을 점차 줄이는 것을 경사법이라한다.\n",
    "\n",
    "최솟값을 찾는 경사법->경사하강법\n",
    "최댓값을 찾는 경사법->경사상승법\n",
    "손실 함수의 부호 반전시키면 이는 의미없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에타는 갱신하는 양을 나타낸다.(신경망 학습에서는 학습률이라함.)\n",
    "한번의 학습으로 얼만큼 학습해야할지 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이라고 한다.\n",
    "\n",
    "학습률 값은 미리 특정 값으로 정해야하는데 일반적으로 값이 너무 크거나작으면 안됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경사 하강법(최솟값)\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x=init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f는 최적화하려는 함수, init_x는x의 초깃값,lr은 학습률, step_num은 반복횟수를 뜻합니다.\n",
    "함수의 기울기는 numerical_gradient로 구하고 기울기에 학습률을 곱한 값을 갱신하는 처리를 step_num번 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2,init_x=init_x,lr=0.1,step_num=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률이 너무 큰 예:lr:10.0\n",
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2,init_x=init_x,lr=10.0,step_num=100)\n",
    "\n",
    "#학습률이 너무 작은 예lr:1e-10\n",
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2,init_x=init_x,lr=1e-10,step_num=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 크면 큰 값으로 발산하고 너무 작으면 거의 갱신되지 않은 채 끝나버린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에서의 기울기\n",
    "여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.\n",
    "가중치를 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐\n",
    "여기서 중요한 점은 미분한 값과 w의 형상은 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numerical_gradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kimju\\OneDrive\\바탕 화면\\Junho\\deep-learning-from-cosmic\\junho\\4주차\\4주차_김준호.ipynb 셀 41\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000040?line=24'>25</a>\u001b[0m net \u001b[39m=\u001b[39m simpleNet()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000040?line=26'>27</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m w: net\u001b[39m.\u001b[39mloss(x, t)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000040?line=27'>28</a>\u001b[0m dW \u001b[39m=\u001b[39m numerical_gradient(f, net\u001b[39m.\u001b[39mW)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000040?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(dW)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimju/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Junho/deep-learning-from-cosmic/junho/4%EC%A3%BC%EC%B0%A8/4%EC%A3%BC%EC%B0%A8_%EA%B9%80%EC%A4%80%ED%98%B8.ipynb#ch0000040?line=31'>32</a>\u001b[0m net\u001b[39m=\u001b[39m simpleNet()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numerical_gradient' is not defined"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)\n",
    "\n",
    "net= simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x=np.array([0.6,0.9])\n",
    "p=net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p)#최댓값의 인덱스\n",
    "\n",
    "t=np.array([0,0,1])#정답레이블\n",
    "net.loss(x,t)\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x,t)#손실함수\n",
    "\n",
    "dW=numerical_gradient(f,net.W)#기울기\n",
    "print(dW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46e0f8da782ee4e5836de201b49d269b788967913c4102a2cdd310d10725f3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
