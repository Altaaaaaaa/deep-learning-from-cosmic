{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4장 신경망 학습\n",
    "\n",
    "학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\n",
    "\n",
    "손실함수: 신경망이 학습할 수 있도록 해주는 지표\n",
    "\n",
    "학습의 목표: 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 데이터에서 학습\n",
    "신경망의 특징 : 데이터를 보고 학습할 수 있다. -> 가중치 매개변수의 값을 데이터를 보고 자동으로 결정         \n",
    "\n",
    "* 선형 분리 가능 문제는 유한 번의 학습을 통해 풀 수 있다. -> 퍼셉트론 수렴 정리로 증명됨\n",
    "비선형 분리 문제는 자동으로 학습할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1 데이터 주도 학습  \n",
    "기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도  \n",
    "신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있는 중요한 특성을 지님  \n",
    "\n",
    "이미지에서 특징을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.\n",
    "특징: 입력 데이터에서 중요한 데이터를 정확하게 추출할 수 있도록 설계된 변환기  \n",
    "\n",
    "이미지의 특징은 보통 벡터로 기술  \n",
    "컴퓨터 비전에서는 SIFT, SURF, HOG 등의 특징을 많이 사용  \n",
    "\n",
    "이미지 데이터를 벡터로 변환 -> 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습  \n",
    "\n",
    "기계학습에서 이미지 벡터를 변환할 때 사용하는 특징은 여전히 사람이 설계한다.\n",
    "\n",
    "딥러닝은 '종단간 기계학습(end-to-end machine learning)'이라고도 한다. '종단간'은 '처음부터 끝까지'라는 의미  데이터에서 목표한 결과를 사람의 개입 없이 얻는다는 뜻이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2 훈련 데이터와 시험 데이터  \n",
    "기계학습 문제는 데이터를 훈련데이터와 시험데이터로 나눠 학습과 실험을 수행하는 것이 일반적  \n",
    "1) 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다.  \n",
    "2) 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가\n",
    "\n",
    "왜 훈련 데이터와 시험 데이터를 나눌까? - 범용적으로 사용할 수 있는 모델을 구하려 하기 때문이다.\n",
    "-> 범용능력을 제대로 평가하기 위해 훈련데이터와 시험데이터를 분리하는 것  \n",
    "\n",
    "범용능력: 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력\n",
    "-> 범용능력을 획득하는 것이 기계하습의 최종 목표  \n",
    "\n",
    "한 데이터셋에만 지나치게 최적화된 상태를 오버피팅(overfitting)이라고 한다. -> 오버피팅을 피하는 것이 기계학습의 중요한 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 손실 함수\n",
    "신경망 학습에서 현재의 상태를 하나의 지표로 표현  \n",
    "그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것  \n",
    "신경망 학습에서 사용하는 지표는 손실 함수(loss function)라고 한다.  \n",
    "손실 함수는 임의의 함수를 사용할 수 있지만, 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용  \n",
    "\n",
    "손실 함수는 신경망 성능의 '나쁨'을 나타나는 지표로 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1 평균 제곱 오차\n",
    "평균 제곱 오차(mean squared error, MSE)   \n",
    "E = $\\frac{1}{2}\\displaystyle\\sum_{k}^{}{(y_k-t_k)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 원소만 1로 하고 그 외에는 0으로 나타내는 표기법을 '원-핫 인코딩' 이라한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# '2'일 확률이 가장 높은 경우\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '7'일 확률이 가장 높은 경우\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y),np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2 교차 엔트로피 오차  \n",
    "E = \n",
    "손실 함수로서 교차 엔트로피 오차(cross entropy error, CEE)도 자주 사용  \n",
    "$-\\displaystyle\\sum_{k}^{}{t_k\\log{y_k}}$      &nbsp;&nbsp;&nbsp;&nbsp; log는 밑이 e인 자연로그다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3 미니배치 학습  \n",
    "훈련 데이터 모두에 대한 손실 함수  \n",
    "$-\\frac{1}{N}\\displaystyle\\sum_{n}^{}\\displaystyle\\sum_{k}^{}{t_{nk}\\log{y_{nk}}}$  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 교차 엔트로피를 모든 훈련 데이터에 대한 손실 함수로 바꾼 수식  \n",
    "마지막에 N으로 나누어 정규화함 -> N으로 나눔으로써 '평균 손실 함수'를 구한 것  \n",
    "\n",
    "모든 데이터를 대상으로 손실 함수의 합을 구하면 오래 걸린다. -> 데이터 일부를 추려 전체의 '근사치'로 이용 가능  \n",
    "-> 미니배치(mini-batch)라 한다.  \n",
    "이러한 방식을 사용하는 학습을 미니배치 학습이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) =\\\n",
    "    load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 10장만 빼내려면 어떻게 하면 될까?    ->    넘파이의 np.random.choice() 함수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = x_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9172, 18408, 20779, 53591, 32632, 24084, 48278, 41493,  3135,\n",
       "       25657])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 이상 60000 미만의 수 중에서 무작위로 10개 추출\n",
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 교차 엔트로피 오차에서 원-핫 인코딩이 아닌 숫자 레이블로 주어졌을 때의 교차 엔트로피\n",
    "def cross_entroy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = t.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 왜 손실 함수를 설정하는가?  \n",
    "\n",
    "궁극적인 목적은 높은 '정확도'를 끌어내는 매개변수 값을 찾는 것\n",
    "\n",
    "'정확도'라는 지표를 놔두고 '손실 함수의 값'이라는 우회적인 방법을 택하는 이유가 뭘까? -> '미분'의 역할에 주목  \n",
    "신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다.  \n",
    "가중치 매개변수의 손실 함수의 미분이란 '기중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나'라는 의미  \n",
    "미분 값이 음수면 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.  \n",
    "미분 값이 양수면 그 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.  \n",
    "미분 값이 0이면 가중치 매개변수를 어느쪽으로 움직여도 손실 함수의 값은 달라지지 않습니다.  \n",
    "\n",
    "정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문  \n",
    "\n",
    "정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다.  \n",
    "-> '계단 함수'를 활성화 함수로 사용하지 않는 이유이기도 하다. 계단 함수의 미분은 대부분의 장소에서 0이다.  \n",
    "그러므로 계단 함수를 이용하면 손실 함수를 지표로 삼는게 의미가 없다.  \n",
    "매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실 함수의 값에는 아무런 변화가 나타나지 않음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.1 미분\n",
    "\n",
    "'미분'은 '특정 순간'의 변화량을 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현 예  -> 반올림 오차 문제를 일으킨다.\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1) \n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)\n",
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.3 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수가 여럿인 함수에 대한 미분을 편미분이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 기울기  \n",
    "모든 변수의 편미분을 벡터로 정리한 것을 '기울기'라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4                     # 0.0001\n",
    "    grad = np.zeros_like(x)      # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.1 경사법(경사 하강법)  \n",
    "최적의 매개변수는 손실 함수가 최솟값이 될 때의 매개변수다.  \n",
    "기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사법  \n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동 -> 또 기울기를 구하고 그방향으로 이동 -> (반복)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x=np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예: lr = 10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr=10.0, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 예: lr = 1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x= init_x, lr=1e-10, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W=np.random.randn(2, 3)    # 정규분포로 초기화\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22028069  0.11545395 -0.21566813]\n",
      " [ 1.68197772  0.07172272  2.31901246]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.38161153 0.13382282 1.95771034]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5443476510666067"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19568043  0.05618749 -0.25186792]\n",
      " [ 0.29352064  0.08428124 -0.37780188]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 학습 알고리즘 구현하기  \n",
    "\n",
    "##### 신경망 학습 절차\n",
    "\n",
    "- 전제  \n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.  \n",
    "신경망 학습은 다음과 같이 4단계로 수행  \n",
    "\n",
    "- 1단계: 미니배치  \n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 선별한 데이터를 미니배치라 한다. 미니배치의 손실 함수 값을 줄이는 것이 목표  \n",
    "\n",
    "- 2단계: 기울기 산출  \n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향 제시  \n",
    "\n",
    "- 3단계: 매개변수 갱신  \n",
    "가중치 매개변수를 기울기 방향으로 조금씩 갱신  \n",
    "\n",
    "- 4단계: 반복\n",
    "1~3 단계 반복  \n",
    "\n",
    "경사 하강법으로 매개변수를 갱신하는 방법이며, 데이터를 미니배치로 무작위로 선정하기 때문에 확률적 경사 하강법(stochastic gradient descent, SGD)라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5.1 2층 신경망 클래스 구현하기\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y= self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "net.params['W1'].shape # (784, 100)\n",
    "net.params['b1'].shape # (100, )\n",
    "net.params['W2'].shape # (100, 10)\n",
    "net.params['b2'].shape # (10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5.2 미니배치 학습 구현하기\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10    # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'b1': array([-7.61759367e-04, -3.29258918e-04, -2.72299499e-04, -1.20235510e-04,\n",
       "        -1.04181948e-03, -1.55159666e-03,  7.42861979e-04, -2.75164269e-05,\n",
       "        -6.96534035e-04, -9.45264853e-04, -1.31851900e-03, -8.12939418e-04,\n",
       "        -3.73856812e-04, -6.61182469e-04, -8.92491616e-04,  1.28454314e-05,\n",
       "        -3.71402589e-04, -2.93358271e-05,  4.52321247e-05, -1.94030643e-04,\n",
       "         4.39044496e-04,  2.64663480e-04,  3.53052492e-04, -3.83903245e-04,\n",
       "        -1.55781439e-04, -1.65616328e-03, -1.17456384e-03, -1.26638491e-04,\n",
       "        -3.68266113e-04, -7.72188529e-04,  7.43381425e-04, -1.13093688e-03,\n",
       "        -1.42815429e-03,  1.67787085e-03,  1.57485021e-04,  1.02747092e-03,\n",
       "         4.66751904e-05, -8.85767961e-04, -6.51350192e-04, -2.04535736e-03,\n",
       "        -1.50375276e-03, -1.50949835e-04, -6.17502491e-04,  1.12517267e-03,\n",
       "        -6.57902475e-04, -3.85343533e-04,  1.97203522e-04, -5.06060525e-04,\n",
       "         1.33572257e-03,  6.13624442e-04]),\n",
       " 'W2': array([[ 5.96446734e-02, -1.28884266e-03,  2.24633080e-03,\n",
       "         -8.80875933e-03,  4.27199754e-02, -1.50590303e-03,\n",
       "          4.95014806e-02, -5.05647311e-02,  4.35279042e-02,\n",
       "         -1.35472128e-01],\n",
       "        [ 5.78111753e-02, -9.35118893e-05, -1.22825083e-03,\n",
       "         -1.00467874e-02,  4.14078797e-02, -1.72608466e-03,\n",
       "          4.79815130e-02, -4.66383692e-02,  4.21919315e-02,\n",
       "         -1.29659495e-01],\n",
       "        [ 6.07765396e-02,  1.00949513e-03, -1.87360429e-03,\n",
       "         -8.50933083e-03,  4.35334535e-02, -1.62408021e-03,\n",
       "          5.04446677e-02, -4.73098944e-02,  4.43575771e-02,\n",
       "         -1.40804823e-01],\n",
       "        [ 6.04648492e-02,  2.00912487e-03,  8.25743707e-04,\n",
       "         -9.93073935e-03,  4.33060702e-02, -2.66966387e-03,\n",
       "          5.01810070e-02, -4.82312966e-02,  4.41258950e-02,\n",
       "         -1.40080990e-01],\n",
       "        [ 5.97143399e-02, -8.37477834e-04, -1.90403646e-04,\n",
       "         -1.03732026e-02,  4.27721117e-02,  1.44899501e-04,\n",
       "          4.95630287e-02, -4.74264599e-02,  4.35819145e-02,\n",
       "         -1.36948750e-01],\n",
       "        [ 6.42107237e-02,  1.25767518e-03, -4.79982365e-05,\n",
       "         -1.13668147e-02,  4.59905773e-02, -2.25269505e-03,\n",
       "          5.32922096e-02, -5.16567546e-02,  4.68611952e-02,\n",
       "         -1.46288118e-01],\n",
       "        [ 6.25006103e-02,  2.93454802e-03, -7.73438318e-04,\n",
       "         -9.31012686e-03,  4.47646332e-02, -2.15982879e-03,\n",
       "          5.18709948e-02, -5.01593511e-02,  4.56121424e-02,\n",
       "         -1.45280184e-01],\n",
       "        [ 6.26550649e-02,  6.36508115e-04,  1.20951251e-03,\n",
       "         -1.03988525e-02,  4.48759941e-02, -1.80171381e-03,\n",
       "          5.20001445e-02, -5.06752731e-02,  4.57258074e-02,\n",
       "         -1.44227192e-01],\n",
       "        [ 5.94780477e-02,  1.44307581e-03,  2.79293544e-05,\n",
       "         -7.42064610e-03,  4.26027150e-02, -2.63886315e-03,\n",
       "          4.93661658e-02, -4.58801781e-02,  4.34088453e-02,\n",
       "         -1.40387091e-01],\n",
       "        [ 6.31142201e-02,  7.66979265e-04, -3.16922725e-03,\n",
       "         -1.12308510e-02,  4.52081430e-02, -2.80758613e-03,\n",
       "          5.23865018e-02, -4.63922118e-02,  4.60643795e-02,\n",
       "         -1.43940347e-01],\n",
       "        [ 6.25658057e-02,  1.88616584e-03,  5.02975639e-05,\n",
       "         -8.66121389e-03,  4.48124412e-02, -2.69893902e-03,\n",
       "          5.19262701e-02, -5.15288204e-02,  4.56600001e-02,\n",
       "         -1.44012007e-01],\n",
       "        [ 5.85685539e-02,  1.22669466e-04,  3.23913520e-03,\n",
       "         -6.98018650e-03,  4.19489896e-02, -1.76382309e-03,\n",
       "          4.86075987e-02, -4.96915201e-02,  4.27418488e-02,\n",
       "         -1.36793266e-01],\n",
       "        [ 5.83358434e-02, -1.38236669e-03,  6.34225776e-04,\n",
       "         -5.69817650e-03,  4.17837842e-02, -4.48135712e-03,\n",
       "          4.84150062e-02, -4.67157481e-02,  4.25737489e-02,\n",
       "         -1.33464960e-01],\n",
       "        [ 6.03630373e-02, -2.72877885e-04,  1.12874716e-04,\n",
       "         -9.04932103e-03,  4.32355539e-02, -1.67438348e-03,\n",
       "          5.00993232e-02, -4.57157468e-02,  4.40543327e-02,\n",
       "         -1.41152792e-01],\n",
       "        [ 6.18247418e-02,  1.09572100e-03,  1.74114039e-04,\n",
       "         -1.09632797e-02,  4.42827802e-02, -2.88750486e-03,\n",
       "          5.13136968e-02, -4.85205235e-02,  4.51214149e-02,\n",
       "         -1.41441161e-01],\n",
       "        [ 6.56853555e-02,  2.69028505e-03, -2.82721634e-03,\n",
       "         -1.23982739e-02,  4.70479795e-02, -2.95638434e-03,\n",
       "          5.45185258e-02, -5.28423990e-02,  4.79385096e-02,\n",
       "         -1.46856382e-01],\n",
       "        [ 6.28163666e-02,  1.38361359e-03, -3.78947520e-04,\n",
       "         -1.07408256e-02,  4.49934529e-02, -1.51114239e-03,\n",
       "          5.21379673e-02, -4.74260225e-02,  4.58455903e-02,\n",
       "         -1.47120053e-01],\n",
       "        [ 6.10016338e-02,  5.84113777e-04,  2.11610289e-04,\n",
       "         -1.05243024e-02,  4.36928340e-02, -5.18842886e-04,\n",
       "          5.06301273e-02, -4.73341298e-02,  4.45205107e-02,\n",
       "         -1.42263555e-01],\n",
       "        [ 6.41049000e-02,  1.93880712e-03, -3.79245841e-03,\n",
       "         -9.43219181e-03,  4.59176179e-02, -1.63685088e-03,\n",
       "          5.32081506e-02, -4.91453502e-02,  4.67870413e-02,\n",
       "         -1.47949666e-01],\n",
       "        [ 5.85647541e-02,  6.63987798e-05,  1.84094229e-03,\n",
       "         -1.01853056e-02,  4.19463501e-02, -1.39788018e-03,\n",
       "          4.86052638e-02, -4.91336190e-02,  4.27402054e-02,\n",
       "         -1.33047110e-01],\n",
       "        [ 5.93131540e-02, -1.52833224e-04, -3.20818270e-03,\n",
       "         -9.30441508e-03,  4.24847842e-02, -2.79891776e-03,\n",
       "          4.92295702e-02, -4.50999494e-02,  4.32892335e-02,\n",
       "         -1.33752444e-01],\n",
       "        [ 6.33968021e-02,  2.86589125e-03,  8.65224283e-04,\n",
       "         -1.16715469e-02,  4.54068762e-02, -4.21876667e-03,\n",
       "          5.26164092e-02, -4.89037858e-02,  4.62669276e-02,\n",
       "         -1.46624031e-01],\n",
       "        [ 6.32828792e-02, -7.74878401e-04, -7.32144023e-04,\n",
       "         -1.14693820e-02,  4.53269144e-02, -3.49309700e-03,\n",
       "          5.25235658e-02, -4.72672654e-02,  4.61852002e-02,\n",
       "         -1.43581793e-01],\n",
       "        [ 6.17689449e-02,  2.61555630e-04,  3.08319024e-04,\n",
       "         -9.59748041e-03,  4.42427342e-02,  5.27512645e-04,\n",
       "          5.12674074e-02, -4.76381705e-02,  4.50805204e-02,\n",
       "         -1.46221343e-01],\n",
       "        [ 6.08760569e-02, -7.53746587e-04,  2.06590327e-04,\n",
       "         -4.92945260e-03,  4.36049926e-02,  1.05094975e-03,\n",
       "          5.05266065e-02, -4.90076025e-02,  4.44295903e-02,\n",
       "         -1.46003985e-01],\n",
       "        [ 6.22516484e-02,  2.02485313e-03,  2.44350205e-03,\n",
       "         -1.28138399e-02,  4.45870276e-02, -9.67706972e-04,\n",
       "          5.16672047e-02, -4.83013570e-02,  4.54320423e-02,\n",
       "         -1.46323374e-01],\n",
       "        [ 6.10103844e-02,  1.92698098e-03,  1.78408661e-04,\n",
       "         -1.23268192e-02,  4.36980035e-02,  2.57723480e-04,\n",
       "          5.06369165e-02, -4.82919913e-02,  4.45258965e-02,\n",
       "         -1.41615503e-01],\n",
       "        [ 6.14966680e-02,  2.65763591e-03, -1.71876305e-03,\n",
       "         -9.08581189e-03,  4.40497553e-02, -1.18207797e-03,\n",
       "          5.10446446e-02, -4.44087414e-02,  4.48844837e-02,\n",
       "         -1.47737793e-01],\n",
       "        [ 5.85440153e-02,  1.54057094e-03,  6.25434879e-03,\n",
       "         -8.30648349e-03,  4.19273835e-02, -3.34597360e-03,\n",
       "          4.85815216e-02, -5.17726353e-02,  4.27202458e-02,\n",
       "         -1.36142993e-01],\n",
       "        [ 6.04407339e-02, -5.86015136e-04,  9.73048953e-04,\n",
       "         -1.08716813e-02,  4.32906518e-02,  1.12434003e-03,\n",
       "          5.01636939e-02, -4.99426638e-02,  4.41106308e-02,\n",
       "         -1.38702739e-01],\n",
       "        [ 6.08731518e-02, -7.28820226e-04, -3.46864171e-03,\n",
       "         -7.38362809e-03,  4.36036556e-02, -1.12432849e-03,\n",
       "          5.05257467e-02, -4.68999089e-02,  4.44284855e-02,\n",
       "         -1.39825712e-01],\n",
       "        [ 6.69862740e-02,  2.91831900e-03, -4.97956263e-03,\n",
       "         -9.75859626e-03,  4.79824922e-02, -2.41220873e-03,\n",
       "          5.56020601e-02, -4.96194735e-02,  4.88909016e-02,\n",
       "         -1.55610206e-01],\n",
       "        [ 6.09492019e-02,  2.60452482e-04,  3.44331758e-03,\n",
       "         -7.32487190e-03,  4.36533488e-02, -1.59654783e-03,\n",
       "          5.05818144e-02, -5.56933251e-02,  4.44781081e-02,\n",
       "         -1.38751498e-01],\n",
       "        [ 6.23560415e-02,  2.47604929e-03, -2.10731624e-03,\n",
       "         -1.07090670e-02,  4.46636829e-02, -8.16623702e-04,\n",
       "          5.17556090e-02, -4.93527529e-02,  4.55091531e-02,\n",
       "         -1.43774776e-01],\n",
       "        [ 6.31929208e-02,  1.18114120e-03,  8.35992298e-04,\n",
       "         -1.18922276e-02,  4.52614164e-02, -2.12762742e-03,\n",
       "          5.24480321e-02, -5.00377621e-02,  4.61183675e-02,\n",
       "         -1.44980253e-01],\n",
       "        [ 6.05813042e-02,  8.95321779e-04, -2.15405206e-03,\n",
       "         -1.08710751e-02,  4.33928911e-02, -3.29729912e-03,\n",
       "          5.02824363e-02, -4.74888546e-02,  4.42145060e-02,\n",
       "         -1.35555178e-01],\n",
       "        [ 5.98393932e-02, -1.77782851e-03,  8.72812402e-04,\n",
       "         -9.94097376e-03,  4.28617784e-02, -9.15459522e-04,\n",
       "          4.96660648e-02, -4.96238883e-02,  4.36726924e-02,\n",
       "         -1.34654591e-01],\n",
       "        [ 6.29832293e-02,  9.66448432e-04, -7.86815719e-04,\n",
       "         -6.86651189e-03,  4.51127311e-02,  5.12258345e-04,\n",
       "          5.22740748e-02, -5.16872554e-02,  4.59661637e-02,\n",
       "         -1.48474322e-01],\n",
       "        [ 5.94132448e-02, -8.39443743e-04,  7.15248702e-04,\n",
       "         -5.40150667e-03,  4.25564048e-02, -9.97452954e-04,\n",
       "          4.93111631e-02, -4.99714824e-02,  4.33602433e-02,\n",
       "         -1.38146419e-01],\n",
       "        [ 5.90792672e-02, -2.48023653e-03,  1.29222344e-03,\n",
       "         -7.60858316e-03,  4.23165972e-02, -8.76815678e-04,\n",
       "          4.90329519e-02, -5.05054019e-02,  4.31168799e-02,\n",
       "         -1.33366882e-01],\n",
       "        [ 6.11939335e-02,  3.21575270e-03, -5.68766647e-04,\n",
       "         -1.32870901e-02,  4.38309162e-02,  2.45102361e-04,\n",
       "          5.07925915e-02, -4.55434273e-02,  4.46617863e-02,\n",
       "         -1.44540798e-01],\n",
       "        [ 5.95817868e-02, -3.74640683e-04,  9.13434901e-04,\n",
       "         -9.09748828e-03,  4.26756573e-02, -3.10094176e-03,\n",
       "          4.94506085e-02, -4.69478241e-02,  4.34831703e-02,\n",
       "         -1.36583763e-01],\n",
       "        [ 6.50239955e-02,  2.63750202e-03,  2.79992436e-03,\n",
       "         -1.05729737e-02,  4.65707166e-02, -3.81768475e-03,\n",
       "          5.39643546e-02, -5.12512546e-02,  4.74524655e-02,\n",
       "         -1.52807045e-01],\n",
       "        [ 6.13380250e-02, -1.40228074e-05, -1.05556593e-03,\n",
       "         -1.18863881e-02,  4.39344629e-02, -1.11423121e-03,\n",
       "          5.09096614e-02, -4.86582019e-02,  4.47667469e-02,\n",
       "         -1.38220486e-01],\n",
       "        [ 5.94392142e-02,  3.32603864e-03, -2.53727512e-03,\n",
       "         -8.39804231e-03,  4.25726379e-02, -5.30326980e-03,\n",
       "          4.93309846e-02, -4.75088851e-02,  4.33781853e-02,\n",
       "         -1.34299588e-01],\n",
       "        [ 5.92326570e-02,  9.03295694e-04, -5.44287158e-04,\n",
       "         -8.75245227e-03,  4.24263978e-02, -1.81546901e-03,\n",
       "          4.91616263e-02, -4.67815586e-02,  4.32294779e-02,\n",
       "         -1.37059688e-01],\n",
       "        [ 5.98240887e-02,  1.53831154e-03, -1.70452720e-03,\n",
       "         -9.88328185e-03,  4.28491981e-02, -1.56517181e-03,\n",
       "          4.96515215e-02, -4.84903381e-02,  4.36608597e-02,\n",
       "         -1.35880660e-01],\n",
       "        [ 5.70811184e-02, -8.39682164e-04,  3.65716175e-04,\n",
       "         -7.15988708e-03,  4.08847625e-02, -5.36689138e-06,\n",
       "          4.73744883e-02, -4.64938876e-02,  4.16582846e-02,\n",
       "         -1.32865546e-01],\n",
       "        [ 6.24978574e-02,  1.62101406e-03, -4.08263530e-03,\n",
       "         -1.19422515e-02,  4.47660897e-02, -2.42994163e-03,\n",
       "          5.18748339e-02, -4.59789973e-02,  4.56142262e-02,\n",
       "         -1.41940195e-01],\n",
       "        [ 6.25020038e-02,  2.63522439e-03, -1.68298391e-03,\n",
       "         -1.25086378e-02,  4.47670581e-02, -1.57331968e-03,\n",
       "          5.18761829e-02, -4.77554211e-02,  4.56150531e-02,\n",
       "         -1.43875160e-01]]),\n",
       " 'b2': array([ 0.12314451,  0.00134256, -0.00158339, -0.01947586,  0.08820402,\n",
       "        -0.00352837,  0.10220751, -0.09751485,  0.08987361, -0.28266975])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad    # 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.00836409, -0.01114407,  0.00212226, ...,  0.00857111,\n",
       "         -0.00053286,  0.00684899],\n",
       "        [ 0.00942402, -0.00456101,  0.00194119, ..., -0.00355991,\n",
       "         -0.00825246,  0.00390427],\n",
       "        [-0.00360707, -0.01381233,  0.00024959, ..., -0.00398042,\n",
       "          0.00305629,  0.00924936],\n",
       "        ...,\n",
       "        [ 0.00338017,  0.01436654, -0.00876726, ..., -0.01065922,\n",
       "          0.00249505,  0.00417061],\n",
       "        [-0.00485614, -0.0165996 , -0.01806027, ...,  0.00416631,\n",
       "         -0.01273886,  0.00619001],\n",
       "        [-0.00061926, -0.00803638,  0.0075701 , ..., -0.02177833,\n",
       "         -0.01448065, -0.00891673]]),\n",
       " 'b1': array([-1.95200032e-04, -3.33134928e-04, -4.13491268e-04, -3.44037982e-04,\n",
       "        -1.92665156e-04, -2.39306302e-04, -5.47249714e-04, -3.80159546e-04,\n",
       "        -2.85522893e-04, -3.53911934e-04, -2.52073964e-04, -1.65703002e-04,\n",
       "        -3.72488988e-04, -3.01868357e-04, -2.92128945e-04, -3.99264303e-04,\n",
       "        -2.89788330e-04, -3.73635194e-04, -4.15568983e-04, -2.07227605e-04,\n",
       "        -4.61933392e-04, -4.70615445e-04, -4.38959675e-04, -3.23725176e-04,\n",
       "        -4.36235865e-04, -1.03629537e-04, -2.50800487e-04, -4.85766785e-04,\n",
       "        -2.12480063e-04, -2.25226227e-04, -4.59837520e-04, -2.60592696e-04,\n",
       "        -8.83766089e-05, -4.96890773e-04, -4.43977654e-04, -5.84193495e-04,\n",
       "        -3.90095925e-04, -2.42779585e-04, -2.64276213e-04, -1.19375377e-04,\n",
       "        -2.08217005e-04, -3.16157770e-04, -3.29199277e-04, -4.12614723e-04,\n",
       "        -2.08454817e-04, -4.02284134e-04, -4.68270543e-04, -2.81641501e-04,\n",
       "        -5.63362846e-04, -4.06909036e-04]),\n",
       " 'W2': array([[ 1.75495073e-02,  1.06544699e-02,  4.86808158e-03,\n",
       "         -4.48823018e-03, -1.14992420e-02,  6.87743343e-03,\n",
       "         -1.43360867e-02,  7.52634812e-03, -8.27346827e-03,\n",
       "          2.71574855e-02],\n",
       "        [-5.43704988e-03,  2.19330260e-03, -1.06753376e-03,\n",
       "         -1.77373453e-02,  3.56334375e-03, -2.89702320e-04,\n",
       "          1.67776513e-02,  2.36280699e-02,  9.54197395e-03,\n",
       "          2.71878088e-02],\n",
       "        [-1.62944639e-02, -8.39487451e-04, -1.42595828e-02,\n",
       "         -2.21578682e-02, -1.38386605e-03,  8.53299665e-03,\n",
       "         -1.23795192e-02,  6.70165814e-03,  5.12681649e-03,\n",
       "          1.56751398e-02],\n",
       "        [ 1.54439277e-02,  6.79407035e-03,  4.30972458e-03,\n",
       "         -2.94512736e-03,  3.14236213e-04,  6.67123685e-03,\n",
       "         -1.34462800e-02,  1.24695665e-02, -1.46763510e-02,\n",
       "          1.77801300e-02],\n",
       "        [-1.35163940e-03,  4.69604941e-03,  1.17438990e-03,\n",
       "         -1.55184740e-02, -1.82551411e-02,  5.09321255e-04,\n",
       "         -2.07497877e-02, -2.01106446e-03, -2.66613306e-03,\n",
       "          2.44258059e-02],\n",
       "        [-4.72196847e-03, -5.10700612e-03, -5.39157126e-03,\n",
       "          1.18517603e-02, -1.78123134e-02, -1.92674974e-02,\n",
       "         -6.67002566e-03, -5.87905422e-03, -7.13610685e-03,\n",
       "          3.52900471e-02],\n",
       "        [ 2.42255385e-03, -1.55514403e-02, -6.21309620e-03,\n",
       "          2.88965629e-03, -2.46264241e-02, -1.56625711e-02,\n",
       "         -2.17551505e-02,  1.17643242e-02, -1.58283590e-02,\n",
       "         -1.04417022e-02],\n",
       "        [ 8.69981776e-03,  1.10714884e-02, -6.12898507e-03,\n",
       "          9.84963110e-04, -6.34960595e-03,  5.04765276e-03,\n",
       "         -2.54087029e-03,  2.46506468e-03, -5.16092329e-03,\n",
       "          2.24607158e-02],\n",
       "        [-3.76825213e-03, -1.48520267e-02,  7.59848107e-03,\n",
       "         -5.15714052e-03, -1.24623019e-02,  1.65721305e-03,\n",
       "          1.22138349e-03,  3.58138831e-03, -1.36291064e-02,\n",
       "          2.21778492e-02],\n",
       "        [ 4.09591791e-03, -2.19696400e-02, -1.76481679e-02,\n",
       "          7.90605366e-03, -1.45244575e-03, -7.58791856e-03,\n",
       "         -6.40660218e-03,  1.26755497e-02, -4.68186367e-03,\n",
       "          2.96619153e-02],\n",
       "        [-1.30407935e-02, -4.30233387e-03, -1.37375722e-02,\n",
       "         -2.45555278e-02, -9.34090260e-06, -8.05126822e-03,\n",
       "         -6.77417037e-03,  8.61727280e-03,  7.72007782e-03,\n",
       "          3.55664164e-02],\n",
       "        [ 4.86902957e-03, -4.31179769e-03, -2.50322256e-03,\n",
       "         -1.90121858e-02, -9.16648916e-03,  1.36283416e-02,\n",
       "         -3.03441671e-03, -7.68230582e-03, -9.26524991e-03,\n",
       "          3.29129501e-02],\n",
       "        [-6.07839107e-03, -7.47301829e-03,  5.05134622e-03,\n",
       "         -3.57836284e-03,  1.22272008e-03,  2.65997114e-03,\n",
       "         -2.47175688e-02, -9.32298570e-03, -8.18248740e-03,\n",
       "          1.68323420e-02],\n",
       "        [-5.64762002e-03,  8.58846208e-03,  4.36421811e-03,\n",
       "         -3.99685619e-03, -1.06817586e-02, -8.14851923e-04,\n",
       "         -1.14351554e-02, -8.26402149e-03, -9.55462669e-03,\n",
       "          2.24885385e-02],\n",
       "        [ 3.31652901e-03,  9.41054474e-03, -1.04368452e-02,\n",
       "         -3.04044406e-03, -6.49845793e-03, -5.02374125e-03,\n",
       "         -1.31256091e-02,  9.51539648e-03, -4.57205194e-03,\n",
       "          2.62034023e-02],\n",
       "        [ 5.21267106e-03, -5.16534698e-03,  5.10785202e-03,\n",
       "         -1.69233395e-03,  7.54606372e-03,  6.08714839e-03,\n",
       "          7.53514092e-03, -7.14934789e-03, -2.29883110e-02,\n",
       "          2.67101015e-02],\n",
       "        [-1.62924791e-03,  7.13497373e-03,  1.84450397e-02,\n",
       "         -1.68297260e-02, -1.21768477e-02,  1.08838969e-02,\n",
       "         -1.46390875e-03,  8.11962493e-03, -8.56750391e-03,\n",
       "          1.95506739e-02],\n",
       "        [-2.03808278e-04,  1.05310544e-02,  3.38214010e-03,\n",
       "         -1.35349857e-02, -9.60723394e-03, -1.16645496e-02,\n",
       "         -1.29674920e-02,  1.18920693e-02, -7.93744951e-03,\n",
       "          1.03239188e-02],\n",
       "        [-7.71639448e-04,  8.25000472e-03, -2.14203452e-03,\n",
       "         -9.93900304e-03, -8.93285489e-03, -1.09571275e-02,\n",
       "         -4.29560879e-03,  6.68766353e-03, -3.78598872e-03,\n",
       "          1.62358754e-02],\n",
       "        [ 2.00905892e-02,  2.18293536e-02,  1.80415529e-03,\n",
       "         -1.97842036e-02, -5.14488009e-03, -2.44468371e-03,\n",
       "         -1.84520752e-02, -1.64195539e-03, -7.12387893e-03,\n",
       "          2.50909522e-02],\n",
       "        [ 8.17028391e-03, -1.47776562e-03,  6.63521668e-03,\n",
       "          1.64519357e-02, -1.85639888e-02,  1.68624600e-02,\n",
       "          4.36675816e-04,  1.85213253e-03,  1.11053616e-03,\n",
       "          1.20655494e-02],\n",
       "        [-3.68822704e-03, -1.41798610e-02,  1.03432194e-02,\n",
       "         -1.12072320e-02, -4.92255910e-03, -8.33193194e-03,\n",
       "         -7.64767861e-03,  1.24426582e-02,  1.89858485e-03,\n",
       "          1.13617512e-02],\n",
       "        [ 1.05907656e-02,  3.53697111e-03,  5.14556181e-03,\n",
       "          3.54173855e-03, -1.70864106e-02, -3.16786228e-04,\n",
       "         -1.22204798e-02,  5.21728867e-03, -1.62907683e-02,\n",
       "          6.24908280e-03],\n",
       "        [-1.14886261e-03,  1.71875447e-02, -6.06821511e-03,\n",
       "         -1.61832730e-02, -1.12194057e-03, -2.13160883e-02,\n",
       "          4.37047442e-03,  4.16350392e-03,  2.44034012e-03,\n",
       "          3.06873278e-02],\n",
       "        [-1.55523087e-02, -2.22297634e-03, -5.06320472e-03,\n",
       "         -3.79424768e-03, -3.87535106e-03, -1.05679389e-03,\n",
       "         -8.73443471e-04, -3.44639495e-03, -1.77703459e-02,\n",
       "          1.33175686e-02],\n",
       "        [ 6.50439159e-03, -1.61038709e-03,  2.53966843e-03,\n",
       "         -1.75668421e-03, -2.30878429e-02, -1.89751453e-02,\n",
       "         -2.92195607e-03,  3.76428131e-03, -6.58268666e-03,\n",
       "          3.87801864e-02],\n",
       "        [-3.33116410e-03,  6.52624561e-03, -1.10833049e-02,\n",
       "          3.36469179e-04, -2.31097169e-02,  1.68612301e-03,\n",
       "         -2.32650305e-03,  1.40719530e-02, -1.01712257e-02,\n",
       "          2.22610984e-02],\n",
       "        [-2.35992491e-02, -3.83181571e-03, -4.84614974e-03,\n",
       "         -4.04371557e-03, -1.30003091e-02, -7.60659149e-03,\n",
       "         -3.30950917e-03, -2.18160980e-03, -9.34236897e-03,\n",
       "          8.12297791e-03],\n",
       "        [ 1.92055107e-02,  1.71619332e-03, -2.90885447e-02,\n",
       "         -1.72534122e-02, -1.53938182e-02,  1.35541758e-03,\n",
       "         -2.01034598e-02,  9.20085162e-03, -1.10901398e-02,\n",
       "          1.82882310e-02],\n",
       "        [-1.39866365e-03,  9.21940252e-04, -1.30037831e-02,\n",
       "         -2.40235969e-02, -1.23091387e-02,  6.88339956e-03,\n",
       "          3.96871288e-03,  1.49296012e-02,  8.61168923e-03,\n",
       "          3.00370343e-02],\n",
       "        [ 1.06129646e-02, -2.97345754e-03,  3.92678180e-03,\n",
       "          4.00072108e-03, -1.94717616e-02, -7.93861963e-03,\n",
       "         -2.85928140e-03, -2.39504753e-03, -6.56941593e-03,\n",
       "          8.42718938e-03],\n",
       "        [-5.12552998e-03, -9.13600412e-03,  5.79083810e-03,\n",
       "         -1.24957281e-02, -1.15140614e-02, -3.16237524e-04,\n",
       "          1.48190566e-03,  1.60183625e-02, -1.62591294e-02,\n",
       "          2.63345695e-02],\n",
       "        [ 9.83177855e-03,  2.14095585e-03, -6.28931053e-03,\n",
       "         -1.37908088e-02, -8.10998861e-03,  1.24448501e-03,\n",
       "         -1.37637687e-03,  2.38543981e-03, -2.23053493e-02,\n",
       "          3.75976551e-02],\n",
       "        [ 2.25034807e-02, -1.33747766e-02,  4.55567879e-03,\n",
       "         -2.05276497e-02, -5.63192362e-03, -3.64833615e-03,\n",
       "          2.30170461e-03, -1.66618781e-03, -7.83155891e-03,\n",
       "          8.16402706e-03],\n",
       "        [ 3.16259092e-03,  6.16186797e-03,  6.05222208e-04,\n",
       "          2.19103958e-03,  4.21566799e-03,  7.58055246e-03,\n",
       "         -4.18122861e-03, -1.06804507e-02, -2.37999522e-02,\n",
       "          1.86846549e-02],\n",
       "        [-6.06446345e-04, -1.10010304e-02, -9.18168636e-03,\n",
       "         -6.11320071e-04, -6.05793135e-03,  2.40098769e-03,\n",
       "          1.34747778e-02,  1.41529032e-02,  1.70092189e-03,\n",
       "          6.32639216e-03],\n",
       "        [-2.68820733e-03,  1.53872999e-02,  4.70497510e-03,\n",
       "         -1.55604854e-02,  6.25492014e-03,  4.33075794e-03,\n",
       "         -7.55765190e-03,  3.55436458e-03, -8.25644384e-03,\n",
       "          1.69867705e-02],\n",
       "        [ 2.54229633e-04,  2.62352393e-03,  1.06806099e-02,\n",
       "         -1.05355144e-02, -7.73914554e-03,  4.14000443e-04,\n",
       "         -9.48094672e-03,  1.44782399e-03, -1.30248404e-02,\n",
       "          2.70485841e-02],\n",
       "        [-1.08545349e-03, -6.89669089e-03,  2.80462787e-03,\n",
       "         -1.76018146e-02, -1.08785277e-02, -1.17467245e-02,\n",
       "         -4.50970553e-03,  1.99787114e-02, -1.56123850e-02,\n",
       "          1.57735719e-02],\n",
       "        [-8.16016078e-03, -3.46851665e-03, -9.90902846e-03,\n",
       "          7.85368920e-04, -2.74563814e-02, -1.51359087e-02,\n",
       "         -1.19998932e-03,  1.99015284e-02, -1.85552601e-02,\n",
       "          2.60170218e-02],\n",
       "        [-1.68388473e-03, -4.08395033e-03, -6.84018970e-03,\n",
       "          2.79641100e-03, -1.45917095e-02, -3.50893307e-03,\n",
       "          1.38868941e-02,  9.25932046e-03, -1.59768573e-03,\n",
       "          4.06032229e-02],\n",
       "        [ 2.23988306e-03, -8.89139128e-03,  1.84032177e-03,\n",
       "         -1.34377258e-02, -1.01768769e-02, -7.32992139e-03,\n",
       "          2.89038793e-03,  6.42725066e-03, -9.39504015e-03,\n",
       "          1.92008489e-02],\n",
       "        [-4.73872265e-03, -4.02089990e-03, -6.91721338e-03,\n",
       "         -1.18510576e-02, -6.66108628e-03, -4.98404261e-03,\n",
       "          6.95508343e-03,  8.19474891e-03, -3.69858958e-03,\n",
       "          2.89696994e-02],\n",
       "        [ 9.08500036e-03,  2.04396947e-02, -5.12768440e-03,\n",
       "         -2.47143387e-02, -2.28828932e-02,  1.07315279e-02,\n",
       "          8.53038638e-03,  4.38619385e-03,  1.89056336e-03,\n",
       "          7.62604210e-03],\n",
       "        [ 1.13090899e-02, -1.20581814e-02, -2.95706305e-03,\n",
       "         -6.18250337e-03, -2.32208414e-02, -9.00767897e-04,\n",
       "         -3.01963558e-03,  8.53957796e-03, -9.05202262e-03,\n",
       "          2.27150056e-02],\n",
       "        [-5.51678895e-03,  1.33564596e-02, -1.57893044e-03,\n",
       "          2.17805942e-03,  1.64971277e-03,  6.35708659e-03,\n",
       "         -8.17639742e-03,  5.62193835e-03, -1.48762946e-02,\n",
       "          1.62298490e-02],\n",
       "        [ 5.44803685e-03,  6.00023462e-03,  3.44213478e-02,\n",
       "          1.34150464e-02,  6.48642483e-03, -4.29487884e-04,\n",
       "         -1.43290424e-02,  2.02023887e-02, -2.56113067e-02,\n",
       "          2.59861817e-03],\n",
       "        [ 3.61983434e-03, -1.09244502e-02, -8.22816852e-03,\n",
       "         -1.08580662e-02,  5.20133512e-03,  6.54513309e-03,\n",
       "         -3.75852795e-03,  6.40268126e-03, -1.71185610e-02,\n",
       "          2.37643417e-02],\n",
       "        [ 1.05868042e-02,  7.46228526e-04,  5.67686940e-03,\n",
       "         -3.38301173e-03, -6.17258080e-03, -8.69410049e-03,\n",
       "         -3.47184120e-03,  3.36010184e-03, -2.09363601e-02,\n",
       "         -1.64947412e-03],\n",
       "        [ 1.06386902e-02,  1.31054206e-02,  8.91373129e-03,\n",
       "         -9.40968775e-03, -1.64543453e-02,  1.16018605e-02,\n",
       "          1.10354335e-02,  1.53543190e-02, -6.24191234e-03,\n",
       "          1.15751748e-02]]),\n",
       " 'b2': array([-7.78403795e-05,  7.91318281e-04,  4.02199974e-03, -1.12429255e-02,\n",
       "        -2.05210886e-02,  1.58718659e-03, -1.09352538e-02,  8.89767995e-03,\n",
       "        -1.51837461e-02,  4.26626687e-02])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.params # 매개변수 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0955870033882205"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1756867723245015,\n",
       " 2.24414741157253,\n",
       " 2.2615685650921824,\n",
       " 2.2092216964409457,\n",
       " 2.2650152356636446,\n",
       " 2.2732139243949834,\n",
       " 2.218619897549307,\n",
       " 2.235618052198526,\n",
       " 2.316292958228103,\n",
       " 2.0955870033882205]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5.3 시험 데이터로 평가하기  \n",
    "손실 함수의 값이 줄어드는 것을 확인할 수 있었다.  \n",
    "이때의 손실 함수의 값이란 '훈련 데이터의 미니배치에 대한 손실 함수'의 값이다.  \n",
    "훈련 데이터의 손실 함수의 값이 작아지는 것은 신경망이 잘 학습하고 있음을 뜻한다.  \n",
    "하지만 이 결과로는 다른 데이터셋에도 비슷한 실력을 발휘할지는 확실하지 않다.  \n",
    "\n",
    "에폭(epoch)  \n",
    "1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당  \n",
    "ex) 훈련 데이터 10,000개를 100개의 미니배치로 학습하는 경우에  \n",
    "확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터는 소진된다.  \n",
    "100회가 1에폭이 된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.11236666666666667, 0.1135\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "    \n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10                  # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10                 # 미니배치  크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \"\n",
    "              + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅이 일어나면 갑자기 정확도가 떨어지는 지점이 생긴다. 그 순간이 오버피팅이 시작되는 순간이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.6 정리  \n",
    "\n",
    "- 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.  \n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가  \n",
    "- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신  \n",
    "- 가중치 매개변수를 갱신할 때 가중치 매개변수의 기울기를 이용, 기울어진 방향으로 가중치의 값을 갱신하는 작업 반복  \n",
    "- 아주 작은 값을 주었을 때의 차분으로 미분한는 것을 수치 미분  \n",
    "- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.  \n",
    "- 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단  \n",
    "\n",
    "5장에서는 오차역전파법이라는 방법으로 기울기를 고속으로 구할 수 있다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88f5a4554b5a9ef949b5de2ca977b5a4bd361180c5b30f32147c76c2c5bcf452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
