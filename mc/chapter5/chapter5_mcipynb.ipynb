{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 오차역전파법  \n",
    "  \n",
    "## 5.1 계산 그래프\n",
    "\n",
    "계산 그래프: 계산 과정을 그래프로 나타낸 것  \n",
    "\n",
    "그래프는 노드(node), 엣지(edge)로 표현\n",
    "\n",
    "### 5.1 계산 그래프로 풀다  \n",
    "계산 그래프는 계산 과정을 노드와 화살표로 표현  \n",
    "노드는 원으로 표기, 안에는 연산 내용을 적는다.  \n",
    "\n",
    "1. 계산 그래프 구성  \n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행  -> 순전파(forward propagation)  \n",
    "\n",
    "순전파가 존재하므로 역전파도 존재한다. 역전파는 미분을 계산할 때 중요한 역할을 한다.  \n",
    "\n",
    "### 5.1.2 국소적 계산\n",
    "계산 그래프를 보면 자신과 직접적으로 관계된 정보만으로 결과를 출력할 수 있다.  \n",
    "ex) 그림에서 사과 2개 사면 사과 x 2만 하면 된다. 다른건 안 한다.  \n",
    "\n",
    "\n",
    "### 5.1.3 왜 계산 그래프로 푸는가?  \n",
    "계산 그래프의 이점  \n",
    "1) 국소적 계산 -> 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화 할 수 있다.\n",
    "2) 중간 계산 결과를 모두 보관 가능  <b>\n",
    "3) 역전파를 통해 미분을 효율적으로 계산할 수 있다.  (그림 5-5 참고)\n",
    "\n",
    "## 5.2 연쇄법칙  \n",
    "'국소적 미분'을 전달하는 원리는 연쇄법칙(chain rule)에 따른 것  \n",
    "\n",
    "#### 5.2.1 계산 그래프의 역전파  \n",
    "국소적인 미분을 상류에서 전달된 값(그림 5-6에서 E)에 곱해 앞쪽 노드로 전달한다.  \n",
    "\n",
    "### 5.2.2 연쇄 법칙이란?  \n",
    "합성 함수: 여러 함수로 구성된 함수  \n",
    "연쇄법칙은 합성 함수의 미분에 대한 성질이다. 다음과 같이 정의할 수 있다.  \n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다._  \n",
    "<br>  \n",
    "\n",
    "## 5.3 역전파  \n",
    "\n",
    "### 5.3.1 덧셈 노드의 역전파  \n",
    "덧셈 노드 역전파는 입력 신호를 다음 노드로 출력할 뿐이므로 그대로 다음 노드로 전달한다.  \n",
    "\n",
    "### 5.3.2 곱셈 노드의 역전파  \n",
    "곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보낸다.  \n",
    "덧셈의 역전파에서는 상류의 값을 그대로 흘려보내서 순방향 입력 신호의 값은 필요하지 않다.  \n",
    "반면에 곱셈의 역전파는 순방향 입력 신호의 값이 필요하다. 곱셈 노드를 구현할 때 순전파의 입력신호를 변수에 저장해둔다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 단순한 계층 구현 -> 계층: 신경망의 기능 단위\n",
    "\n",
    "# 5.4.1 곱셈 계층\n",
    "class MuLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y): # 순전파\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): # 역전파              # dout(미분)\n",
    "        dx = dout * self.y # x와 y를 바꾼다.\n",
    "        dy = dout * self.x # y와 x를 바꾼다.\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MuLayer()\n",
    "mul_tax_layer = MuLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.2 덧셈 계층\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MuLayer()\n",
    "mul_orange_layer = MuLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MuLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num) # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax) # (4)\n",
    "\n",
    "# 역전파\n",
    "dpirce = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice) # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
    "\n",
    "print(price) # 715\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 활성화 함수 계층 구현하기\n",
    "\n",
    "# 5.5.1 ReLU 계층\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask는 True/False로 구성된 넘파이 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.2 Sigmoid 계층\n",
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine / Softmax 계층 구현하기  \n",
    "\n",
    "### 5.6.1 Affine 계층  \n",
    "신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서는 어파인 변환(Affine transformation)이라 한다.  \n",
    "책에서는 어파인 변환을 수행하는 처리를 'Affine 계층'이라는 이름으로 구현한다.  \n",
    "\n",
    "### 5.6.2 배치용 Affine 계층  \n",
    "데이터 N개를 묶어 순전파하는 경우, 즉 배치용 Affine 계층  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3,], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis = 0) # 0이면 열방향으로 합, 1이면 행방향으로 합\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = B\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.w) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층  \n",
    "신경망에서 수행하는 작업은 학습과 추론 두 가지다.  \n",
    "추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다.  \n",
    "신경망은 추론할 때는 마지막 Affine 계층의 출력을 인식 결과로 이용한다.  \n",
    "신경망에서 정규화하지 않는 출력 결과(그림 5-28에서는 Softmax 앞의 Affine 계층의 출력)를 '점수'라 한다.  \n",
    "즉, 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되므로 Softmax 계층은 필요 없다.  \n",
    "신경망을 학습할 때는 Softmax 계층이 필요하다.  \n",
    "\n",
    "소프트맥스 함수의 손실 함수로 교차 엔트로피 오차를 사용하면 역전파가 말끔히 떨어진다.  \n",
    "\n",
    "회귀의 출력층에서 사용하는 항등함수의 손실함수로 평균 제곱 오차를 이용하면 역전파의 결과가 말끔히 떨어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.x = None    # 정답 레이블(원-핫 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self. y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx=(self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림  \n",
    "\n",
    "신경망 학습의 순서  \n",
    "\n",
    "* 전제  \n",
    "신경망에서는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.  \n",
    "\n",
    "* 1단계-미니배치  \n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 한다. 그 미니배치의 손실 함수를 줄이는 것이 목표  \n",
    "\n",
    "* 2단계-기울기 산출  \n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시  \n",
    "\n",
    "* 3단계-매개변수 갱신  \n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신  \n",
    "\n",
    "* 4단계-반복  \n",
    "1~3단계 반복  \n",
    "\n",
    "<br>\n",
    "전 장에서는 수치 미분을 사용해서 기울기를 구했다. 수치 미분은 구현하기 쉽지만, 계산이 오래 걸렸다.  \n",
    "오차역전파법을 이용하면 수치 미분보다 빠르게 기울기를 구할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers import *\n",
    "from gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):       # x: 입력 데이터, t: 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차역전법이 있으면 수치 미분이 쓸모 없는 것이 아니다.  \n",
    "오차역전법을 정확히 구현했는지 확인하기 위해서는 수치 미분이 필요하다.  \n",
    "수치미분과 오차역전법으로 구한 기울기가 일치함을 확인하는 작업을 기울기 확인(gradient check)이라고 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:7.581209748527943e-07\n",
      "b1:7.6027188125434954e-06\n",
      "W2:6.070309449540768e-09\n",
      "b2:1.3969978891054113e-07\n"
     ]
    }
   ],
   "source": [
    "# 기울기 확인\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17656666666666668 0.1819\n",
      "0.90285 0.907\n"
     ]
    }
   ],
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다.\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88f5a4554b5a9ef949b5de2ca977b5a4bd361180c5b30f32147c76c2c5bcf452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
