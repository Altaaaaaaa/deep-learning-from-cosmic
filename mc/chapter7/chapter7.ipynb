{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter7 합성곱 신경망(CNN)\n",
    "합성곱 신경망(Convolutional Neural Network, CNN)은 이미지 인식과 음성 인식 등 다양한 곳에서 사용된다.  \n",
    "특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 다 CNN을 기초로 한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 전체 구조  \n",
    "CNN에는 합성곱 계층(convolutional layer), 풀링 계층(pooling layer)가 새롭게 등장한다.  \n",
    "앞서 구현한 신경망들은 인접하는 계층의 모든 뉴런과 결합되어 있었다. 이를 완전연결(fully-connected, 전결합)이라고 한다.  \n",
    "Conv-ReLU-(Pooling) 흐름으로 연결된다. (풀링 계층은 생략되기도 한다.)  \n",
    "지금까지 신경망에서 구현하듯이 Affine-ReLU 구성을 사용할 수 있다. 마지막 출력 계층에서는 Affine-Softmax 조합을 그대로 사용한다.  \n",
    "\n",
    "7.2 합성곱 계층  \n",
    "CNN에서는 패딩(padding), 스트라이드(stride) 등 CNN 고유의 용어가 등장한다.  \n",
    "또, 각 계층 사이에는 3차원 데이터같이 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과 다르다.  \n",
    "\n",
    "7.2.1 완전연결 계층의 문제점  \n",
    "완전연결 계층의 문제점: 데이터의 형상이 무시된다. ex) 데이터가 이미지면 1차원 데이터로 평탄화해서 처리한다.  \n",
    "-> 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급하여 형상에 담긴 정보를 살릴 수 없다.  \n",
    "(공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 밀접하게 관련되어 있거나, 거리가 먼 픽셀끼리는 별 연관이 없는 등)\n",
    "\n",
    "합성곱 계층은 형상을 유지한다.  \n",
    "CNN에서는 합성곱 계층의 입출력 데이터를 특징 맵(feature map)이라고도 한다. 합성곱 계층의 입력 데이터를 입력 특징 맵(input feature map), 출력 데이터를 출력 특징 맵(output feature map)이라고 한다.  \n",
    "\n",
    "7.2.2 합성곱 연산  \n",
    "합성곱 연산은 이미지 처리에서 필터 연산에 해당한다.  \n",
    "데이터와 필터의 형상을 (높이(height), 너비(width))로 표기  \n",
    "필터를 커널이라 칭하기도 한다.  \n",
    "합성곱 연산은 필터의 윈도우(window)를 일정 간격으로 이동해가며 입력 데이터에 적용  \n",
    "입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구한다.(단일 곱셈-누산(fused multiply-add, FMA)이라 한다.)\n",
    "\n",
    "완전연결 신경망에서는 가중치 매개변수와 편향이 존재한다. CNN에서는 필터의 매개변수가 그동안의 '가중치'에 해당한다.  \n",
    "편향은 필터를 적용한 후의 데이터에 더해진다. 편향은 항상 하나만 존재한다. 그 하나의 값을 필터를 적용한 모든원소에 더하는 것  \n",
    "\n",
    "7.2.3 패딩  \n",
    "패딩(padding): 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값으로 채우는 것  \n",
    "패딩은 주로 출력 크기를 조정할 목적으로 사용  \n",
    "합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서 문제가 될 수 있다. -> 크기가 점점 작아져 어느 시점에서 1이 되버림 -> 패딩 사용  \n",
    "\n",
    "7.2.4 스트라이드\n",
    "스트라이드(stride): 필터를 적용하는 위치의 간격 ex) 스트라이드를 2로하면 필터를 적용하는 윈도우가 두 칸씩 이동  \n",
    "\n",
    "스트라이드를 키우면 출력 크기는 작아진다. 한편 패딩을 크게 하면 출력 크기가 커진다.  (수식은 책 참고)  \n",
    "\n",
    "7.2.5 3차원 데이터의 합성곱 연산  \n",
    "2차원일 때와 비교하면, 길이 방향(채널 방향)으로 특징 맵이 늘어났다. 채널 쪽으로 특징 맵이 여러 개 있다면, 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더해서 하나의 출력을 얻는다.  \n",
    "3차원의 합성곱은 입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다. 한편, 필터 자체의 크기는 원하는 값으로 설정할 수 있다.(단, 모든 채널의 필터가 같은 크기여야 한다.)  \n",
    "\n",
    "7.2.6 블록으로 생각하기  \n",
    "3차원 데이터를 다차원 배열로 나타낼 떄는 (채널(channel), 높이(height), 너비(width) 순서로 쓴다. (C, H, W) 필터도 같은 순서로 쓴다.(C, FH, FW)  \n",
    "\n",
    "합성곱 연산의 출력으로 다수의 채널을 내보내려면 어떻게 해야 할까?  \n",
    "-> 필터(가중치)를 다수 사용한다.  \n",
    "\n",
    "7.2.7 배치 처리  \n",
    "각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장한다. 데이터를 (데이터 수, 채널 수, 높이, 너비) 순으로 저장  \n",
    "신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이뤄진다.  \n",
    "\n",
    "7.3 풀링 계층  \n",
    "풀링(pooling): 세로, 가로 방향의 공간을 줄이는 연산  \n",
    "최대 풀링(max pooling): 윈도우에서 최댓값을 구하는 연산    \n",
    "풀링의 윈도우 크기오 스트라이드는 같은 값으로 설정하는 것이 보통\n",
    "\n",
    "번외) 평균 풀링(average pooling)이 있다. 이미지 인식 분야에서는 주로 최대 풀링을 사용한다.  \n",
    "\n",
    "7.3.1 풀링 계층의 특징\n",
    "* 학습해야할 매개변수가 없다. -> 풀링은 대상 영역에서 최댓값이나 평균을 취하는 명확한 처리이므로 특별히 학습할 것이 없다.  \n",
    "* 채널 수가 변하지 않는다. -> 채널마다 독립적으로 계산하기 때문  \n",
    "* 입력의 변화에 영향을 적게 받는다. -> 입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않는다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4 합성곱/풀링 계층 구현하기  \n",
    "\n",
    "7.4.1 4차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(10, 1, 28, 28) # 무작위로 데이터 생성\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape)\n",
    "print(x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73233039, 0.99501435, 0.77746818, 0.62023231, 0.66525591,\n",
       "        0.63412701, 0.95303226, 0.26018855, 0.4831213 , 0.43909244,\n",
       "        0.4665478 , 0.11732176, 0.0760272 , 0.38143979, 0.37853677,\n",
       "        0.24341915, 0.45145638, 0.272714  , 0.30566027, 0.0375253 ,\n",
       "        0.39665656, 0.53557198, 0.52984653, 0.02878381, 0.53564892,\n",
       "        0.13980036, 0.02912207, 0.55071236],\n",
       "       [0.59252797, 0.82451917, 0.60266233, 0.76449308, 0.55283731,\n",
       "        0.41992464, 0.18877372, 0.23960692, 0.60237394, 0.21558001,\n",
       "        0.10127722, 0.02544551, 0.59154691, 0.59856095, 0.64246313,\n",
       "        0.32279441, 0.80573701, 0.48706637, 0.41093993, 0.44049385,\n",
       "        0.81605686, 0.02642658, 0.2882356 , 0.21032401, 0.1165467 ,\n",
       "        0.01295579, 0.09512919, 0.39799929],\n",
       "       [0.36722646, 0.94365911, 0.85960058, 0.30510692, 0.97522227,\n",
       "        0.75549009, 0.83211334, 0.26394289, 0.0749231 , 0.27792072,\n",
       "        0.74749603, 0.0136184 , 0.7423647 , 0.08916325, 0.72454541,\n",
       "        0.07618776, 0.28993869, 0.1201781 , 0.72612049, 0.71254562,\n",
       "        0.27339349, 0.13083324, 0.87356001, 0.891009  , 0.89551573,\n",
       "        0.014954  , 0.11224253, 0.70034137],\n",
       "       [0.07042455, 0.45396784, 0.98223749, 0.29010696, 0.49993257,\n",
       "        0.76108496, 0.17229635, 0.65986787, 0.10555583, 0.96919049,\n",
       "        0.12351324, 0.3436424 , 0.29187214, 0.02107447, 0.69007794,\n",
       "        0.58007899, 0.7178774 , 0.75850891, 0.15392659, 0.34088509,\n",
       "        0.36974362, 0.33894753, 0.47519351, 0.07170451, 0.13879887,\n",
       "        0.42341629, 0.8905112 , 0.97163607],\n",
       "       [0.32009146, 0.59755169, 0.63940911, 0.41729903, 0.14759092,\n",
       "        0.0763724 , 0.8840468 , 0.33765165, 0.95072883, 0.32918476,\n",
       "        0.26590404, 0.61895417, 0.90475038, 0.33356998, 0.28387418,\n",
       "        0.58376071, 0.94531413, 0.92204995, 0.78674522, 0.05167504,\n",
       "        0.57280858, 0.67504033, 0.18617656, 0.04211852, 0.7603603 ,\n",
       "        0.85443115, 0.47432868, 0.44120279],\n",
       "       [0.96659082, 0.54189538, 0.40129866, 0.33290586, 0.34038189,\n",
       "        0.07336977, 0.46987795, 0.96853204, 0.97925064, 0.75848383,\n",
       "        0.6429819 , 0.58011788, 0.12704974, 0.50104171, 0.02923837,\n",
       "        0.9917722 , 0.01222671, 0.38541237, 0.43270142, 0.94865409,\n",
       "        0.97766889, 0.47864562, 0.27849796, 0.83997807, 0.86757749,\n",
       "        0.30713777, 0.92693783, 0.66881232],\n",
       "       [0.12614697, 0.9790875 , 0.92681969, 0.90807802, 0.33373131,\n",
       "        0.64764783, 0.18639677, 0.1478026 , 0.38563628, 0.35097579,\n",
       "        0.28768472, 0.52041672, 0.29659764, 0.81051945, 0.02627159,\n",
       "        0.72960984, 0.88784468, 0.87702744, 0.73209741, 0.43379053,\n",
       "        0.34089964, 0.53751887, 0.39952637, 0.21540388, 0.06582315,\n",
       "        0.48516426, 0.43649871, 0.90606777],\n",
       "       [0.56999857, 0.06525073, 0.11657315, 0.72367063, 0.97287862,\n",
       "        0.02018575, 0.45103857, 0.55663401, 0.88931651, 0.69599258,\n",
       "        0.83155884, 0.34061805, 0.49108334, 0.99605752, 0.48085283,\n",
       "        0.14687319, 0.88666859, 0.02423467, 0.02773746, 0.94967779,\n",
       "        0.88232251, 0.27881293, 0.2449557 , 0.52439392, 0.7821378 ,\n",
       "        0.60806941, 0.96397444, 0.12045449],\n",
       "       [0.46755755, 0.2003794 , 0.89459082, 0.7785252 , 0.09856252,\n",
       "        0.78028981, 0.54687764, 0.95846124, 0.62122051, 0.54928946,\n",
       "        0.16760635, 0.01975628, 0.03172207, 0.79235253, 0.10236503,\n",
       "        0.1311947 , 0.84614866, 0.01901962, 0.55068379, 0.34687988,\n",
       "        0.21824882, 0.9770684 , 0.26622996, 0.47481748, 0.05173541,\n",
       "        0.52250507, 0.76594783, 0.6841836 ],\n",
       "       [0.71416526, 0.42633791, 0.6859831 , 0.58476135, 0.18454903,\n",
       "        0.40539293, 0.53315357, 0.21537239, 0.69621466, 0.22011764,\n",
       "        0.72082   , 0.83858988, 0.9720859 , 0.75526691, 0.69474063,\n",
       "        0.7970346 , 0.42754229, 0.4350526 , 0.57079972, 0.20655977,\n",
       "        0.41313666, 0.47115835, 0.68423584, 0.81447833, 0.97569264,\n",
       "        0.11819786, 0.1737942 , 0.6571129 ],\n",
       "       [0.01124364, 0.54342701, 0.02289691, 0.89660044, 0.7402513 ,\n",
       "        0.59213742, 0.63057813, 0.57080823, 0.78063287, 0.52305208,\n",
       "        0.03657783, 0.48283608, 0.23970508, 0.29871939, 0.05841691,\n",
       "        0.34062634, 0.35303204, 0.71378523, 0.95231176, 0.81547661,\n",
       "        0.18655559, 0.31620921, 0.56739438, 0.39004368, 0.78687527,\n",
       "        0.86226352, 0.39721146, 0.80318452],\n",
       "       [0.73215636, 0.51707248, 0.25191419, 0.03652907, 0.79380123,\n",
       "        0.34281305, 0.92716192, 0.14657261, 0.00815547, 0.59592394,\n",
       "        0.69681149, 0.5881791 , 0.55857668, 0.36269428, 0.69792807,\n",
       "        0.02855007, 0.49501789, 0.24892777, 0.76666808, 0.46576809,\n",
       "        0.56187401, 0.86347677, 0.80264592, 0.72204492, 0.28155888,\n",
       "        0.28033851, 0.92599398, 0.21382266],\n",
       "       [0.45772158, 0.124515  , 0.91358737, 0.21014774, 0.4019081 ,\n",
       "        0.76339101, 0.32509045, 0.84561197, 0.48186182, 0.62397724,\n",
       "        0.40321216, 0.64886621, 0.1779031 , 0.77951565, 0.01969005,\n",
       "        0.20881455, 0.52482954, 0.81383018, 0.87443788, 0.02943451,\n",
       "        0.2028323 , 0.4274329 , 0.58487892, 0.49295854, 0.69079951,\n",
       "        0.44598704, 0.06368451, 0.66360915],\n",
       "       [0.43535975, 0.78526945, 0.48039607, 0.17324882, 0.74049129,\n",
       "        0.68039167, 0.7008812 , 0.53141288, 0.28732337, 0.44089058,\n",
       "        0.62371188, 0.40277725, 0.9436976 , 0.3423161 , 0.66206916,\n",
       "        0.5087369 , 0.05456907, 0.29181256, 0.38188604, 0.10167604,\n",
       "        0.76426393, 0.40300388, 0.24688945, 0.24585943, 0.56478667,\n",
       "        0.74988084, 0.3833304 , 0.86793326],\n",
       "       [0.09133284, 0.60082126, 0.61158282, 0.11826215, 0.80703462,\n",
       "        0.67318331, 0.08516693, 0.86299627, 0.99508009, 0.84372586,\n",
       "        0.75786624, 0.56597175, 0.9101048 , 0.64416141, 0.02427563,\n",
       "        0.81423054, 0.89179135, 0.9409039 , 0.47006661, 0.0413459 ,\n",
       "        0.74225831, 0.76860245, 0.60556776, 0.23883481, 0.4509036 ,\n",
       "        0.16496936, 0.32888098, 0.6618713 ],\n",
       "       [0.8303077 , 0.53042676, 0.2829671 , 0.3658374 , 0.50169394,\n",
       "        0.99402507, 0.71597317, 0.39696024, 0.91532326, 0.8075004 ,\n",
       "        0.83790493, 0.81755918, 0.84161492, 0.87273175, 0.2816374 ,\n",
       "        0.99884407, 0.63553646, 0.00354672, 0.18786196, 0.36357556,\n",
       "        0.53722683, 0.59354792, 0.39268141, 0.13029928, 0.25280079,\n",
       "        0.97823996, 0.76900179, 0.12213441],\n",
       "       [0.02641619, 0.80672923, 0.04369193, 0.12368594, 0.68846881,\n",
       "        0.01157521, 0.45566264, 0.83728624, 0.12314769, 0.40062579,\n",
       "        0.96967341, 0.20018029, 0.33744793, 0.35522902, 0.70544407,\n",
       "        0.92799603, 0.15317479, 0.60870184, 0.19237824, 0.43702691,\n",
       "        0.83199937, 0.96205744, 0.31480545, 0.99692782, 0.73698673,\n",
       "        0.57122911, 0.79882596, 0.19481259],\n",
       "       [0.63815626, 0.07478476, 0.03988806, 0.09411889, 0.58904955,\n",
       "        0.02734978, 0.90896783, 0.04696766, 0.5453136 , 0.24327631,\n",
       "        0.15869145, 0.65281969, 0.90825124, 0.40249466, 0.30535054,\n",
       "        0.63596557, 0.75894021, 0.37558149, 0.10265504, 0.60030502,\n",
       "        0.87047797, 0.95052134, 0.54521298, 0.6209781 , 0.60674468,\n",
       "        0.23498886, 0.95500703, 0.49497277],\n",
       "       [0.23874108, 0.31863178, 0.66467089, 0.06037356, 0.44854098,\n",
       "        0.6574853 , 0.28198317, 0.21012272, 0.18301074, 0.94057068,\n",
       "        0.05290997, 0.53922939, 0.92980187, 0.64965289, 0.52954643,\n",
       "        0.13862707, 0.63068299, 0.45906466, 0.77304796, 0.93208088,\n",
       "        0.25878975, 0.34285093, 0.68883329, 0.06832372, 0.58879511,\n",
       "        0.98346382, 0.01085021, 0.4802405 ],\n",
       "       [0.98993139, 0.77009506, 0.68243125, 0.72624341, 0.60083497,\n",
       "        0.65779192, 0.50763116, 0.01016412, 0.19070342, 0.30422765,\n",
       "        0.41307107, 0.42640527, 0.14953037, 0.03354763, 0.22641606,\n",
       "        0.3153567 , 0.0784187 , 0.61406588, 0.66428539, 0.1991302 ,\n",
       "        0.63050649, 0.96168849, 0.88167908, 0.59466112, 0.83741953,\n",
       "        0.47736426, 0.53760596, 0.74873145],\n",
       "       [0.083952  , 0.15303659, 0.1305571 , 0.01450386, 0.15819055,\n",
       "        0.09566202, 0.48210988, 0.32410503, 0.52803233, 0.36120702,\n",
       "        0.96788286, 0.64050619, 0.4546336 , 0.06733111, 0.19576084,\n",
       "        0.19628859, 0.03235815, 0.41620432, 0.20580632, 0.22366794,\n",
       "        0.24092499, 0.8434326 , 0.72222186, 0.08992349, 0.12920243,\n",
       "        0.93669952, 0.33903335, 0.74369366],\n",
       "       [0.09178692, 0.45065698, 0.59139021, 0.0160037 , 0.56133595,\n",
       "        0.96170166, 0.06742666, 0.19552323, 0.15807148, 0.95823323,\n",
       "        0.45751874, 0.80829482, 0.78603919, 0.06228087, 0.23135502,\n",
       "        0.27881403, 0.11496785, 0.89973143, 0.93844449, 0.20133185,\n",
       "        0.80216563, 0.89197668, 0.73815259, 0.45351929, 0.73810387,\n",
       "        0.33588793, 0.72462137, 0.74001335],\n",
       "       [0.36131683, 0.80066286, 0.16122481, 0.55879397, 0.44348852,\n",
       "        0.4417256 , 0.91191031, 0.53426008, 0.16614607, 0.39417642,\n",
       "        0.35791928, 0.95354545, 0.99596671, 0.50968058, 0.59133468,\n",
       "        0.23257046, 0.48588395, 0.4300857 , 0.33766748, 0.47877834,\n",
       "        0.75589526, 0.03068423, 0.25581099, 0.63918336, 0.07053275,\n",
       "        0.47366561, 0.07449443, 0.73650515],\n",
       "       [0.26240398, 0.98346696, 0.10685457, 0.05804814, 0.76361058,\n",
       "        0.50739745, 0.68825826, 0.50756745, 0.06144714, 0.81943235,\n",
       "        0.48831345, 0.66141213, 0.5764669 , 0.12545501, 0.38178649,\n",
       "        0.2232246 , 0.94945658, 0.29139577, 0.56181001, 0.97521837,\n",
       "        0.10830512, 0.08510843, 0.61127949, 0.59473608, 0.78459726,\n",
       "        0.87064311, 0.86907439, 0.08641242],\n",
       "       [0.91852254, 0.28627099, 0.77401693, 0.87002585, 0.92293711,\n",
       "        0.79218432, 0.81941319, 0.0387021 , 0.96049966, 0.0625267 ,\n",
       "        0.05389198, 0.38290117, 0.19405114, 0.30076066, 0.85795743,\n",
       "        0.45947557, 0.10294626, 0.73483335, 0.53027927, 0.80853921,\n",
       "        0.78742105, 0.33893239, 0.70294257, 0.80148955, 0.01136263,\n",
       "        0.01187953, 0.74465868, 0.52285632],\n",
       "       [0.58989462, 0.31049003, 0.24441504, 0.8756694 , 0.27071078,\n",
       "        0.83069615, 0.52175522, 0.89991583, 0.51849492, 0.89593755,\n",
       "        0.0654274 , 0.58147082, 0.19037683, 0.56251956, 0.78694007,\n",
       "        0.26981254, 0.30688207, 0.15872753, 0.6979876 , 0.34135178,\n",
       "        0.57885624, 0.10777979, 0.83472902, 0.02864348, 0.51266466,\n",
       "        0.631735  , 0.69639445, 0.38792866],\n",
       "       [0.81637325, 0.25655945, 0.36022329, 0.39345744, 0.30077879,\n",
       "        0.8474726 , 0.48556815, 0.06158267, 0.69602817, 0.28580263,\n",
       "        0.13320264, 0.78578738, 0.32628697, 0.3670033 , 0.25430042,\n",
       "        0.41981839, 0.41208415, 0.3875539 , 0.87468415, 0.68355625,\n",
       "        0.79427783, 0.82199505, 0.42761084, 0.2654152 , 0.55277505,\n",
       "        0.0957124 , 0.72679773, 0.57900856],\n",
       "       [0.03594553, 0.8952979 , 0.9324672 , 0.55685459, 0.72788679,\n",
       "        0.85164983, 0.2385263 , 0.76003011, 0.77684996, 0.7406065 ,\n",
       "        0.90331081, 0.28104719, 0.80915307, 0.05294701, 0.9075991 ,\n",
       "        0.07609257, 0.66872068, 0.3264762 , 0.93720213, 0.64307055,\n",
       "        0.10584303, 0.5311288 , 0.81597454, 0.78834676, 0.72802706,\n",
       "        0.00703296, 0.43088496, 0.88701323]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0] # 또는 x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4.2 im2col로 데이터 전개하기  \n",
    "합성곱 연산을 무지성으로 만들면 for문을 겹겹이 써야한다.  \n",
    "넘파이에 for문을 사용하면 성능이 떨어지는 단점이 존재  \n",
    "im2col은 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는(펼치는) 함수다. ex) 3차원 입력 데이터에 im2col을 적용한면 2차원 행렬로 바뀐다.(정확히는 배치 안의 데이터 수까지 포함한 4차우너 데이터를 2차원으로 변환)  \n",
    "\n",
    "스트라이드를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만, 실제 상황에서는 영역이 겹치는 경우가 대부분이다.  \n",
    "필터 적용 영역이 겹치게 되면 im2col로 전개한 후의 원소 수가 원래 블록의 원소 수보다 많아진다.  \n",
    "-> im2col을 사용해 구현하면 메모리를 더 많이 소비한다.  \n",
    "하지만, 컴퓨터는 큰 행렬을 묶어서 계산하는 데 탁월 ex) 행렬 계산 라이브러리등을 사용하면 빠르게 계산 가능  \n",
    "\n",
    "번외) im2coldms 'image to column'의 약자다. '이미지에서 행렬로'라는 의미다.  \n",
    "\n",
    "im2col로 입력 데이터를 전개한 다음 합성곱 계층의 필터(가중치)를 1열로 전개하고, 두 행렬의 내적을 계산한다. -> Affine 계층에서 한 것과 거의 같다.  \n",
    "\n",
    "im2col 방식으로 출력한 결과는 2차원 행렬이다. CNN은 4차원 배열로 저장하므로 2차원인 출력 데이터를 4차원으로 변형한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4.3 합성곱 계층 구현하기  \n",
    "\n",
    "im2col(input_data, filter_h, filter_w, stride=1, pad=0)  \n",
    "* input_data: (데이터 수, 채널 수, 높이, 너비)의 4차원 배열로 이뤄진 입력 데이터  \n",
    "* filter_h: 필터의 높이  \n",
    "* filter_w: 필터의 너비  \n",
    "* stride: 스트라이드  \n",
    "* pad: 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7) # (데이터 수, 채널 수, 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # (9, 75)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7) # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) # (90, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # 필터 전개\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4.4 풀링 계층 구현하기  \n",
    "풀링 계층 구현도 합성곱 계층과 마찬가지로 im2col을 사용해 입력 데이터를 전개한다. 단, 풀링의 경우엔 채널 쪽이 독립적이라는 점이 합성곱 계층 때와 다르다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 최댓값 (2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.5 CNN 구현하기  \n",
    "손글씨 숫자 인식하는 CNN 조립\n",
    "\n",
    "Convolution - ReLU - Polling - Affine - ReLU - Affine - Softmax 순으로 흐르게 한다.\n",
    "\n",
    "초기화 할 때 받는 인수  \n",
    "* input_dim: 필터 수\n",
    "* conv_param: 합성곱 계층의 하이퍼파라미터(딕셔너리).\n",
    "    * filter_num: 필터 수\n",
    "    * filter_size: 필터 크기\n",
    "    * stride: 스트라이드\n",
    "    * pad: 패딩\n",
    "    * hidden_size: 은닉층(완전연결)의 뉴런 수\n",
    "    * output_size: 출력층(완전연결)의 뉴런 수\n",
    "    * weight_init_std: 초기화 때의 가중치 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5,\n",
    "                             'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        filter_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "                            filter_stride +1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "        \n",
    "        \n",
    "        # 가중치 매개변수 초기화하는 부분\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)        \n",
    "\n",
    "        # CNN 구성하는 계층\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2', self.params['b2']])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29910439661077\n",
      "=== epoch:1, train acc:0.087, test acc:0.089 ===\n",
      "train loss:2.2949154042212996\n",
      "train loss:2.293763351708817\n",
      "train loss:2.2830421955040907\n",
      "train loss:2.272683998554791\n",
      "train loss:2.2607556233051493\n",
      "train loss:2.258591616557919\n",
      "train loss:2.232407391461841\n",
      "train loss:2.2054357770347535\n",
      "train loss:2.1931990030004194\n",
      "train loss:2.1189635749152336\n",
      "train loss:2.1064439683626865\n",
      "train loss:2.0658346323312515\n",
      "train loss:2.065919633874741\n",
      "train loss:1.9573454881329553\n",
      "train loss:1.8658073080659008\n",
      "train loss:1.8033430200740088\n",
      "train loss:1.7250520485073446\n",
      "train loss:1.7643764426780235\n",
      "train loss:1.5377769848270857\n",
      "train loss:1.5370258501166425\n",
      "train loss:1.4022274579098084\n",
      "train loss:1.226374248663039\n",
      "train loss:1.3821492394942771\n",
      "train loss:1.2912895877053636\n",
      "train loss:1.0864571035744506\n",
      "train loss:0.9784125632214062\n",
      "train loss:0.9152172402851526\n",
      "train loss:0.8277509791242004\n",
      "train loss:0.8586330331959509\n",
      "train loss:0.9266463704458555\n",
      "train loss:0.9767606609109012\n",
      "train loss:0.6242158899677177\n",
      "train loss:0.7429798694173441\n",
      "train loss:0.8048382473737681\n",
      "train loss:0.7802798025869913\n",
      "train loss:0.7439277263496009\n",
      "train loss:0.5432747591772509\n",
      "train loss:0.6161555598477464\n",
      "train loss:0.5624781478652716\n",
      "train loss:0.6212559971346227\n",
      "train loss:0.6481815978436538\n",
      "train loss:0.5008830342874799\n",
      "train loss:0.44284978859076285\n",
      "train loss:0.8343422037038963\n",
      "train loss:0.6379660494795254\n",
      "train loss:0.6489023981674602\n",
      "train loss:0.568197232506488\n",
      "train loss:0.4927440710034114\n",
      "train loss:0.47980325278746816\n",
      "train loss:0.6068454271925887\n",
      "train loss:0.5014225796503716\n",
      "train loss:0.42105856908603456\n",
      "train loss:0.40136601048724474\n",
      "train loss:0.431951240730154\n",
      "train loss:0.5326046306160124\n",
      "train loss:0.3476593995185703\n",
      "train loss:0.48123928724092385\n",
      "train loss:0.514154581070715\n",
      "train loss:0.4644423583933312\n",
      "train loss:0.4065605524227332\n",
      "train loss:0.42874048835859857\n",
      "train loss:0.33658019086134333\n",
      "train loss:0.40115240140163594\n",
      "train loss:0.5131045657475723\n",
      "train loss:0.3659019454259173\n",
      "train loss:0.43762051503414795\n",
      "train loss:0.3736429786742395\n",
      "train loss:0.36554088948239255\n",
      "train loss:0.3060262359859658\n",
      "train loss:0.6651743320410829\n",
      "train loss:0.3527086623225303\n",
      "train loss:0.41856610899180013\n",
      "train loss:0.27641030720623705\n",
      "train loss:0.4299455926641748\n",
      "train loss:0.1919872556438689\n",
      "train loss:0.24398288614407743\n",
      "train loss:0.350681897407204\n",
      "train loss:0.2645373034551433\n",
      "train loss:0.4544638640775137\n",
      "train loss:0.45899166411574754\n",
      "train loss:0.32914759369591834\n",
      "train loss:0.49452561428318054\n",
      "train loss:0.3708796534322973\n",
      "train loss:0.31851115556736437\n",
      "train loss:0.47369789803683693\n",
      "train loss:0.34148144128704366\n",
      "train loss:0.5161562095354066\n",
      "train loss:0.3291877758350806\n",
      "train loss:0.5181808630076702\n",
      "train loss:0.5183812682280379\n",
      "train loss:0.2675686609035796\n",
      "train loss:0.4655161818267733\n",
      "train loss:0.5170859015506608\n",
      "train loss:0.2709621850644862\n",
      "train loss:0.2823483421945943\n",
      "train loss:0.367302788521153\n",
      "train loss:0.29454448834861213\n",
      "train loss:0.21460582003875822\n",
      "train loss:0.21045981261080804\n",
      "train loss:0.4863257625967716\n",
      "train loss:0.2482547542319318\n",
      "train loss:0.3621870133089702\n",
      "train loss:0.5413532365543834\n",
      "train loss:0.2994574181431628\n",
      "train loss:0.3358143152107552\n",
      "train loss:0.20890149412377532\n",
      "train loss:0.23388547712275895\n",
      "train loss:0.4557018029640972\n",
      "train loss:0.3433721542603421\n",
      "train loss:0.38006252531660173\n",
      "train loss:0.24495127596258093\n",
      "train loss:0.25828450774774486\n",
      "train loss:0.34535909662399644\n",
      "train loss:0.4667991462710594\n",
      "train loss:0.34243074663543427\n",
      "train loss:0.33032653536038503\n",
      "train loss:0.32226005305652355\n",
      "train loss:0.38976254995141185\n",
      "train loss:0.42248303392062114\n",
      "train loss:0.2950781014120819\n",
      "train loss:0.2782797191809843\n",
      "train loss:0.4125214524659018\n",
      "train loss:0.42684613457640347\n",
      "train loss:0.3788650795014013\n",
      "train loss:0.22413738463072838\n",
      "train loss:0.47518151711490314\n",
      "train loss:0.5391495833973733\n",
      "train loss:0.43381256806849294\n",
      "train loss:0.3033397042019901\n",
      "train loss:0.31283823252119114\n",
      "train loss:0.40448516012868885\n",
      "train loss:0.18474912788419606\n",
      "train loss:0.2466909525672598\n",
      "train loss:0.3051029408223617\n",
      "train loss:0.3068326931744139\n",
      "train loss:0.33396715325022697\n",
      "train loss:0.3510622655322535\n",
      "train loss:0.2120929278169001\n",
      "train loss:0.497585193582499\n",
      "train loss:0.4208692121933328\n",
      "train loss:0.22150001212658335\n",
      "train loss:0.34673656056359436\n",
      "train loss:0.30664306225432775\n",
      "train loss:0.24716613272874582\n",
      "train loss:0.20138200973329462\n",
      "train loss:0.4206148251157122\n",
      "train loss:0.23023729407608\n",
      "train loss:0.19332818533923035\n",
      "train loss:0.3091833414555996\n",
      "train loss:0.23120152947425981\n",
      "train loss:0.30129006921965407\n",
      "train loss:0.22422523423271645\n",
      "train loss:0.3612928918931015\n",
      "train loss:0.40101027639527037\n",
      "train loss:0.2854332128622021\n",
      "train loss:0.3698842099635387\n",
      "train loss:0.4810682668255638\n",
      "train loss:0.4710976925222898\n",
      "train loss:0.6662209233010259\n",
      "train loss:0.2205900637308324\n",
      "train loss:0.22475112889620735\n",
      "train loss:0.34258181991104286\n",
      "train loss:0.3029445941692237\n",
      "train loss:0.2339226737898095\n",
      "train loss:0.332941959598468\n",
      "train loss:0.2996958825595826\n",
      "train loss:0.3120723727158216\n",
      "train loss:0.2630899438640107\n",
      "train loss:0.2716779881957844\n",
      "train loss:0.30944940858060804\n",
      "train loss:0.23781013176212665\n",
      "train loss:0.20661962304044418\n",
      "train loss:0.3790176660928737\n",
      "train loss:0.39206095003328856\n",
      "train loss:0.36935354712460394\n",
      "train loss:0.3076624749041717\n",
      "train loss:0.3532473684197661\n",
      "train loss:0.2527939703146986\n",
      "train loss:0.3934327349037035\n",
      "train loss:0.35336818572925827\n",
      "train loss:0.38263395312277565\n",
      "train loss:0.23072249423844046\n",
      "train loss:0.3029319772145117\n",
      "train loss:0.35620655608436663\n",
      "train loss:0.3672299256729887\n",
      "train loss:0.1771445454228035\n",
      "train loss:0.28584829910238446\n",
      "train loss:0.23054532844878697\n",
      "train loss:0.2471966204554955\n",
      "train loss:0.2789136557339602\n",
      "train loss:0.3443336061795144\n",
      "train loss:0.24330292954611943\n",
      "train loss:0.24852977682910488\n",
      "train loss:0.17992293405092774\n",
      "train loss:0.16038710010050297\n",
      "train loss:0.26128948020969806\n",
      "train loss:0.24217549448294734\n",
      "train loss:0.2228860363093231\n",
      "train loss:0.19821744332337118\n",
      "train loss:0.33393907417983115\n",
      "train loss:0.3097089952641858\n",
      "train loss:0.23198850953997352\n",
      "train loss:0.19600615013649608\n",
      "train loss:0.18528644460709223\n",
      "train loss:0.2672786999033535\n",
      "train loss:0.3339629807096317\n",
      "train loss:0.33976804300139735\n",
      "train loss:0.40085224627404203\n",
      "train loss:0.21644666727100936\n",
      "train loss:0.29754547315167007\n",
      "train loss:0.43832092433477476\n",
      "train loss:0.3524213567155481\n",
      "train loss:0.3227086511620217\n",
      "train loss:0.3025263915774691\n",
      "train loss:0.20231184182194398\n",
      "train loss:0.24763087205167597\n",
      "train loss:0.20575117666832862\n",
      "train loss:0.2535929695602257\n",
      "train loss:0.3732508278867234\n",
      "train loss:0.2877889362824122\n",
      "train loss:0.371220977157056\n",
      "train loss:0.21458458181346374\n",
      "train loss:0.2564219214980328\n",
      "train loss:0.17004937847647436\n",
      "train loss:0.310190388863214\n",
      "train loss:0.22419489443450186\n",
      "train loss:0.2930357116716291\n",
      "train loss:0.4169400538344106\n",
      "train loss:0.24613481213967806\n",
      "train loss:0.2559254401891754\n",
      "train loss:0.15952309919058322\n",
      "train loss:0.2773011781893585\n",
      "train loss:0.3222013790220764\n",
      "train loss:0.14920563513159113\n",
      "train loss:0.14293074346035176\n",
      "train loss:0.30139057260866103\n",
      "train loss:0.3391738251238713\n",
      "train loss:0.3214508722787601\n",
      "train loss:0.27206343126526655\n",
      "train loss:0.16733814807510616\n",
      "train loss:0.2830960637855874\n",
      "train loss:0.3198564191521623\n",
      "train loss:0.20259233771820923\n",
      "train loss:0.24225936400807796\n",
      "train loss:0.30640776834468936\n",
      "train loss:0.06906661946134018\n",
      "train loss:0.22076367333470162\n",
      "train loss:0.19971188154173616\n",
      "train loss:0.29066802103225564\n",
      "train loss:0.2065812448391278\n",
      "train loss:0.36399953324848455\n",
      "train loss:0.3234039986445832\n",
      "train loss:0.2156443905612229\n",
      "train loss:0.21188344939926437\n",
      "train loss:0.2157417061039597\n",
      "train loss:0.26505479844920776\n",
      "train loss:0.2421782541084629\n",
      "train loss:0.19094152304513\n",
      "train loss:0.2052718788433029\n",
      "train loss:0.1526487794706095\n",
      "train loss:0.22077969170445175\n",
      "train loss:0.25112306659162675\n",
      "train loss:0.11100950519255819\n",
      "train loss:0.33755993295515324\n",
      "train loss:0.20802558035787194\n",
      "train loss:0.19116453798285973\n",
      "train loss:0.23042168643765243\n",
      "train loss:0.21807935291951516\n",
      "train loss:0.38847825097178784\n",
      "train loss:0.19964159604242404\n",
      "train loss:0.18745581138597933\n",
      "train loss:0.14875996960175977\n",
      "train loss:0.09710322941452781\n",
      "train loss:0.2691540274520433\n",
      "train loss:0.19709072470644692\n",
      "train loss:0.20750820491355518\n",
      "train loss:0.23217084680708955\n",
      "train loss:0.18990588135225955\n",
      "train loss:0.28046116980799357\n",
      "train loss:0.23813683493709234\n",
      "train loss:0.2883951679489899\n",
      "train loss:0.31910717360266005\n",
      "train loss:0.3220416692465555\n",
      "train loss:0.21505286384558125\n",
      "train loss:0.15120641205241744\n",
      "train loss:0.2686666043418895\n",
      "train loss:0.19496642693606336\n",
      "train loss:0.21266527030464696\n",
      "train loss:0.18516783681610519\n",
      "train loss:0.2384462203984796\n",
      "train loss:0.18868237250648134\n",
      "train loss:0.2600103352672118\n",
      "train loss:0.32129942850416776\n",
      "train loss:0.21710923210786956\n",
      "train loss:0.19084085780044652\n",
      "train loss:0.37260245020179517\n",
      "train loss:0.2108230877175771\n",
      "train loss:0.1394659851353064\n",
      "train loss:0.26461080812753934\n",
      "train loss:0.1775294952503236\n",
      "train loss:0.2890537063490778\n",
      "train loss:0.3345373821213247\n",
      "train loss:0.2347439120677393\n",
      "train loss:0.16616726422952852\n",
      "train loss:0.2345348796813792\n",
      "train loss:0.2530695751176657\n",
      "train loss:0.33452878213477194\n",
      "train loss:0.19175866514030346\n",
      "train loss:0.16703633254740655\n",
      "train loss:0.20127253539251908\n",
      "train loss:0.20192078691985962\n",
      "train loss:0.162870978873021\n",
      "train loss:0.19851711127313215\n",
      "train loss:0.3901301258195754\n",
      "train loss:0.2348531661320998\n",
      "train loss:0.2339690424842101\n",
      "train loss:0.2009214406373459\n",
      "train loss:0.1479668104966377\n",
      "train loss:0.15376590359569256\n",
      "train loss:0.17322251992104498\n",
      "train loss:0.2378222256670096\n",
      "train loss:0.1437492299799824\n",
      "train loss:0.2588677582745818\n",
      "train loss:0.23772447271094277\n",
      "train loss:0.1223586825193828\n",
      "train loss:0.14904668351971354\n",
      "train loss:0.21769696015614964\n",
      "train loss:0.17892740814728905\n",
      "train loss:0.3627191583193111\n",
      "train loss:0.22610442650916196\n",
      "train loss:0.13993241155789227\n",
      "train loss:0.20716790468907817\n",
      "train loss:0.19313326369602987\n",
      "train loss:0.1936499600625719\n",
      "train loss:0.15585034909400466\n",
      "train loss:0.13388840843219957\n",
      "train loss:0.20531642783753212\n",
      "train loss:0.15308969337082068\n",
      "train loss:0.2379674014851471\n",
      "train loss:0.1426574794271338\n",
      "train loss:0.12848600709467203\n",
      "train loss:0.21103755543281572\n",
      "train loss:0.2310662172156044\n",
      "train loss:0.11849944922967229\n",
      "train loss:0.1534738169134842\n",
      "train loss:0.18942975091691683\n",
      "train loss:0.21536260174196137\n",
      "train loss:0.2688327549139513\n",
      "train loss:0.1516137888285527\n",
      "train loss:0.1453799932652629\n",
      "train loss:0.13740008823268862\n",
      "train loss:0.23397951765709277\n",
      "train loss:0.0941143150383976\n",
      "train loss:0.15848648210130864\n",
      "train loss:0.08980378383387327\n",
      "train loss:0.35288851030680995\n",
      "train loss:0.37995669393958303\n",
      "train loss:0.2716844052252747\n",
      "train loss:0.2851653910261573\n",
      "train loss:0.20625336588140136\n",
      "train loss:0.19918859909056574\n",
      "train loss:0.13363375175692807\n",
      "train loss:0.1583370079436955\n",
      "train loss:0.10484414710169793\n",
      "train loss:0.23645473947551324\n",
      "train loss:0.10494123715427711\n",
      "train loss:0.18500407367842656\n",
      "train loss:0.23677101592396416\n",
      "train loss:0.20941776776758828\n",
      "train loss:0.24634532636793355\n",
      "train loss:0.17495680250162343\n",
      "train loss:0.12323740546193022\n",
      "train loss:0.12821094086449145\n",
      "train loss:0.08396612152141367\n",
      "train loss:0.13838828885657214\n",
      "train loss:0.13723731126903027\n",
      "train loss:0.18553914405723598\n",
      "train loss:0.10777460240033626\n",
      "train loss:0.06889236020103874\n",
      "train loss:0.2496445265378368\n",
      "train loss:0.1739370541356262\n",
      "train loss:0.1671049531202269\n",
      "train loss:0.298782189775239\n",
      "train loss:0.18759977596115704\n",
      "train loss:0.14549217821924437\n",
      "train loss:0.15311252867948988\n",
      "train loss:0.22690985982354397\n",
      "train loss:0.1252441635023137\n",
      "train loss:0.21637524576041017\n",
      "train loss:0.31677206247906126\n",
      "train loss:0.15016031156315676\n",
      "train loss:0.11684225460641803\n",
      "train loss:0.09562886472196132\n",
      "train loss:0.0946897162558508\n",
      "train loss:0.2000404379238789\n",
      "train loss:0.18538992499779228\n",
      "train loss:0.18051234914703923\n",
      "train loss:0.24397981640106498\n",
      "train loss:0.18676300098945828\n",
      "train loss:0.23673324412387353\n",
      "train loss:0.06540804841613093\n",
      "train loss:0.1355681457409183\n",
      "train loss:0.11106005906590037\n",
      "train loss:0.20232968300544335\n",
      "train loss:0.28150505891748984\n",
      "train loss:0.24881950506105477\n",
      "train loss:0.11737702027623838\n",
      "train loss:0.1134514111090458\n",
      "train loss:0.13312906894050194\n",
      "train loss:0.18219976939700244\n",
      "train loss:0.12575994601542465\n",
      "train loss:0.1501781958564866\n",
      "train loss:0.12441815227730972\n",
      "train loss:0.099980577160296\n",
      "train loss:0.11695131676295939\n",
      "train loss:0.13275762361584229\n",
      "train loss:0.12514336338747062\n",
      "train loss:0.1354206624849569\n",
      "train loss:0.11396399060141825\n",
      "train loss:0.20579362382478725\n",
      "train loss:0.0803196918058488\n",
      "train loss:0.09182947794316153\n",
      "train loss:0.19452488144600552\n",
      "train loss:0.17043356385320718\n",
      "train loss:0.12991417675547443\n",
      "train loss:0.25041920755541786\n",
      "train loss:0.09499608201134208\n",
      "train loss:0.15533974668092193\n",
      "train loss:0.16174120831638306\n",
      "train loss:0.17534427813750142\n",
      "train loss:0.12092255156835506\n",
      "train loss:0.20787932668144887\n",
      "train loss:0.13741770652667662\n",
      "train loss:0.2366160125348827\n",
      "train loss:0.18791789703082107\n",
      "train loss:0.14278994546511586\n",
      "train loss:0.11829812097806953\n",
      "train loss:0.15791247157176874\n",
      "train loss:0.15635040534054206\n",
      "train loss:0.2607665681189586\n",
      "train loss:0.08916920866293086\n",
      "train loss:0.14175683182782933\n",
      "train loss:0.13812307773787016\n",
      "train loss:0.31648113950561285\n",
      "train loss:0.08610378794048247\n",
      "train loss:0.18173687009395267\n",
      "train loss:0.18155981273775168\n",
      "train loss:0.11407607352660806\n",
      "train loss:0.20397091848036566\n",
      "train loss:0.12419928499601056\n",
      "train loss:0.09967500611741893\n",
      "train loss:0.2352174442033698\n",
      "train loss:0.10686819364997958\n",
      "train loss:0.20922366610103219\n",
      "train loss:0.09552023269638811\n",
      "train loss:0.05834516814985892\n",
      "train loss:0.1610478712773652\n",
      "train loss:0.10729691321858191\n",
      "train loss:0.1273585896479157\n",
      "train loss:0.13426372686741794\n",
      "train loss:0.17040473695131142\n",
      "train loss:0.12597035871429207\n",
      "train loss:0.1787476786180452\n",
      "train loss:0.18348636517415673\n",
      "train loss:0.06766947337305187\n",
      "train loss:0.044896413843566865\n",
      "train loss:0.16052390225658025\n",
      "train loss:0.14045579571220895\n",
      "train loss:0.2765790680968624\n",
      "train loss:0.21210040647826442\n",
      "train loss:0.17214514122042668\n",
      "train loss:0.10631753497384652\n",
      "train loss:0.13068343677335842\n",
      "train loss:0.062375489078523874\n",
      "train loss:0.060355757365455995\n",
      "train loss:0.11155172233579429\n",
      "train loss:0.19280438238987102\n",
      "train loss:0.16840665835581742\n",
      "train loss:0.14831242611200243\n",
      "train loss:0.09581924590885844\n",
      "train loss:0.06988222128701793\n",
      "train loss:0.07903445174301352\n",
      "train loss:0.13368326680096812\n",
      "train loss:0.21058974032275704\n",
      "train loss:0.20570843758629315\n",
      "train loss:0.08563058913593871\n",
      "train loss:0.10695691595810546\n",
      "train loss:0.21772939626495802\n",
      "train loss:0.15402045756442143\n",
      "train loss:0.03814518421810084\n",
      "train loss:0.09930127441302741\n",
      "train loss:0.15029188280518152\n",
      "train loss:0.31564341769588905\n",
      "train loss:0.07363011898533289\n",
      "train loss:0.10712816483789032\n",
      "train loss:0.16920372460651462\n",
      "train loss:0.1878778156051878\n",
      "train loss:0.11911980807823876\n",
      "train loss:0.1576352778369792\n",
      "train loss:0.11039934674631736\n",
      "train loss:0.1947191938078476\n",
      "train loss:0.10685685530273098\n",
      "train loss:0.10197132511026968\n",
      "train loss:0.11154080707420846\n",
      "train loss:0.04962094993341524\n",
      "train loss:0.10729800388962396\n",
      "train loss:0.09851414610113944\n",
      "train loss:0.14002266531825358\n",
      "train loss:0.09546002245238562\n",
      "train loss:0.04340944117549129\n",
      "train loss:0.11449916093201817\n",
      "train loss:0.1014863329259976\n",
      "train loss:0.11258982293604686\n",
      "train loss:0.11795560689187214\n",
      "train loss:0.14020649466758484\n",
      "train loss:0.16292261086830795\n",
      "train loss:0.200071244722997\n",
      "train loss:0.1348029722157807\n",
      "train loss:0.13346947285376984\n",
      "train loss:0.09920674858305106\n",
      "train loss:0.0916856771773147\n",
      "train loss:0.1544584785854522\n",
      "train loss:0.11934286155013346\n",
      "train loss:0.2037736229344474\n",
      "train loss:0.09423251061529242\n",
      "train loss:0.1870124922697135\n",
      "train loss:0.13402097018491868\n",
      "train loss:0.14351922215179017\n",
      "train loss:0.16470495439119975\n",
      "train loss:0.12443133176974458\n",
      "train loss:0.07294069036844003\n",
      "train loss:0.2532990055399997\n",
      "train loss:0.10104738394043906\n",
      "train loss:0.09744755646432064\n",
      "train loss:0.08099951802311141\n",
      "train loss:0.16136634382171697\n",
      "train loss:0.1996591295229037\n",
      "train loss:0.11949368328832843\n",
      "train loss:0.10530341029890261\n",
      "train loss:0.14227304510108196\n",
      "train loss:0.09848292591623223\n",
      "train loss:0.08155879417607202\n",
      "train loss:0.12940631981068051\n",
      "train loss:0.12392611519337059\n",
      "train loss:0.13739848582804653\n",
      "train loss:0.02853845813841214\n",
      "train loss:0.12329640547141782\n",
      "train loss:0.12059574079942813\n",
      "train loss:0.11534611962457234\n",
      "train loss:0.1285995301358898\n",
      "train loss:0.11299723021150936\n",
      "train loss:0.10757911280366624\n",
      "train loss:0.06499602286015432\n",
      "train loss:0.12841814833901333\n",
      "train loss:0.17466353940323515\n",
      "train loss:0.06805060434582064\n",
      "train loss:0.0745879342862545\n",
      "train loss:0.17527295414714594\n",
      "train loss:0.08243409731324243\n",
      "train loss:0.13636768958061382\n",
      "train loss:0.061086717661107535\n",
      "train loss:0.09764495239274548\n",
      "train loss:0.15343422558610886\n",
      "train loss:0.09557773170985602\n",
      "train loss:0.08358734325231171\n",
      "train loss:0.11588659395337249\n",
      "train loss:0.1432104431144381\n",
      "train loss:0.09072839326676735\n",
      "train loss:0.06945317108208518\n",
      "train loss:0.08548907242000679\n",
      "train loss:0.06167602521549039\n",
      "train loss:0.11436091636141196\n",
      "train loss:0.042183818494491085\n",
      "train loss:0.10017692195493089\n",
      "train loss:0.05696138712072228\n",
      "train loss:0.09983313197737193\n",
      "train loss:0.11344926446944748\n",
      "train loss:0.10964189981060798\n",
      "train loss:0.08934954993830083\n",
      "train loss:0.06635337195295976\n",
      "train loss:0.18925318084707\n",
      "train loss:0.11260803053576787\n",
      "train loss:0.10150739765180158\n",
      "train loss:0.08468664942865237\n",
      "train loss:0.07853433649673396\n",
      "train loss:0.14000398213451956\n",
      "train loss:0.08374562154308213\n",
      "train loss:0.09475151695000142\n",
      "train loss:0.09423384984883139\n",
      "train loss:0.09320104490578844\n",
      "train loss:0.11915328031377317\n",
      "train loss:0.11339118321284081\n",
      "train loss:0.06254483709413068\n",
      "train loss:0.13311519219594803\n",
      "train loss:0.1308851564559612\n",
      "train loss:0.09836196724488844\n",
      "train loss:0.1314602666570034\n",
      "train loss:0.08917365811105156\n",
      "train loss:0.07141227147656984\n",
      "train loss:0.09309993386974985\n",
      "=== epoch:2, train acc:0.964, test acc:0.964 ===\n",
      "train loss:0.10188106437095891\n",
      "train loss:0.12394503949107484\n",
      "train loss:0.15553723421468885\n",
      "train loss:0.04766213950547511\n",
      "train loss:0.0890505876158601\n",
      "train loss:0.11714045366580116\n",
      "train loss:0.06792597028213941\n",
      "train loss:0.09456844043233285\n",
      "train loss:0.2044866013686265\n",
      "train loss:0.07594425103191409\n",
      "train loss:0.07953658265455782\n",
      "train loss:0.07598186247655335\n",
      "train loss:0.08080650264639803\n",
      "train loss:0.07808251079527723\n",
      "train loss:0.10735466735118973\n",
      "train loss:0.050230093130552864\n",
      "train loss:0.16546395126031155\n",
      "train loss:0.1492444578103676\n",
      "train loss:0.12029954601394775\n",
      "train loss:0.09721142508311617\n",
      "train loss:0.07195999262948029\n",
      "train loss:0.07214606293173655\n",
      "train loss:0.09390523843540194\n",
      "train loss:0.18096197288859991\n",
      "train loss:0.10909281828590098\n",
      "train loss:0.05305483514437918\n",
      "train loss:0.05900449612606928\n",
      "train loss:0.07349215759308998\n",
      "train loss:0.10025266289907585\n",
      "train loss:0.1557462676978032\n",
      "train loss:0.06864791413562908\n",
      "train loss:0.09049825571501317\n",
      "train loss:0.03497500077634351\n",
      "train loss:0.14974258692037692\n",
      "train loss:0.10377912333373546\n",
      "train loss:0.14298508319714218\n",
      "train loss:0.04945852942200224\n",
      "train loss:0.06929277764398843\n",
      "train loss:0.05549216674754676\n",
      "train loss:0.14943724560364405\n",
      "train loss:0.1247445348168104\n",
      "train loss:0.025854128716825985\n",
      "train loss:0.06475717765039728\n",
      "train loss:0.050796018405045856\n",
      "train loss:0.15468291826316694\n",
      "train loss:0.07581290827653198\n",
      "train loss:0.039232307023870455\n",
      "train loss:0.08670165917714108\n",
      "train loss:0.06360879054224433\n",
      "train loss:0.06712736684642052\n",
      "train loss:0.08239919958091495\n",
      "train loss:0.12024633799863256\n",
      "train loss:0.05936355032844431\n",
      "train loss:0.1379338405963903\n",
      "train loss:0.11016808700230607\n",
      "train loss:0.09070314831033849\n",
      "train loss:0.12404006444718892\n",
      "train loss:0.05523612394830864\n",
      "train loss:0.10654197940234901\n",
      "train loss:0.0791352853094578\n",
      "train loss:0.03126839121925966\n",
      "train loss:0.08861623521611231\n",
      "train loss:0.1106137226342747\n",
      "train loss:0.14902609623915192\n",
      "train loss:0.14372190274261812\n",
      "train loss:0.05556244654614802\n",
      "train loss:0.05033703952466702\n",
      "train loss:0.05343079826773547\n",
      "train loss:0.044822429280529805\n",
      "train loss:0.06744010047762557\n",
      "train loss:0.09963356118729266\n",
      "train loss:0.06105553856013645\n",
      "train loss:0.11118095714321048\n",
      "train loss:0.13293200978534328\n",
      "train loss:0.11037025939680627\n",
      "train loss:0.0488456182219255\n",
      "train loss:0.05697059314234437\n",
      "train loss:0.05906551925962376\n",
      "train loss:0.12709390077448357\n",
      "train loss:0.14532750092236543\n",
      "train loss:0.05499141819499337\n",
      "train loss:0.0519139610200366\n",
      "train loss:0.03900144650327324\n",
      "train loss:0.03431841595511819\n",
      "train loss:0.1476834946979052\n",
      "train loss:0.14216862276438522\n",
      "train loss:0.08181471463382028\n",
      "train loss:0.07953782818899384\n",
      "train loss:0.11471243999526125\n",
      "train loss:0.1115535565589416\n",
      "train loss:0.06481567248684278\n",
      "train loss:0.20129710896795505\n",
      "train loss:0.11101031650179914\n",
      "train loss:0.18618589460309412\n",
      "train loss:0.07764495213392797\n",
      "train loss:0.10685793582517604\n",
      "train loss:0.09426266736663567\n",
      "train loss:0.21308606632855656\n",
      "train loss:0.044236424151182056\n",
      "train loss:0.05450996825826232\n",
      "train loss:0.06576775953615488\n",
      "train loss:0.06927860549132119\n",
      "train loss:0.16634976022352194\n",
      "train loss:0.10299567797518851\n",
      "train loss:0.04322554701425476\n",
      "train loss:0.11850163729825777\n",
      "train loss:0.0500134678627858\n",
      "train loss:0.127746797840176\n",
      "train loss:0.061345988021274646\n",
      "train loss:0.026305988458172905\n",
      "train loss:0.08617824947993079\n",
      "train loss:0.028293261096349425\n",
      "train loss:0.06481514343144702\n",
      "train loss:0.13144470182129356\n",
      "train loss:0.03432755837411274\n",
      "train loss:0.23159202500186332\n",
      "train loss:0.06231671967242232\n",
      "train loss:0.10670571390465607\n",
      "train loss:0.053500742862369395\n",
      "train loss:0.04081040562819661\n",
      "train loss:0.05194836130319888\n",
      "train loss:0.021279240165602283\n",
      "train loss:0.07980984367905954\n",
      "train loss:0.09632982538988678\n",
      "train loss:0.13525815915279313\n",
      "train loss:0.06473855828945164\n",
      "train loss:0.0833412094253876\n",
      "train loss:0.09604801002003169\n",
      "train loss:0.059146129528550466\n",
      "train loss:0.057142960340429066\n",
      "train loss:0.087786767741049\n",
      "train loss:0.06250540718852551\n",
      "train loss:0.03390359215255071\n",
      "train loss:0.194399868672288\n",
      "train loss:0.0729312519315361\n",
      "train loss:0.09584095418060415\n",
      "train loss:0.08500880132365937\n",
      "train loss:0.038717281110585436\n",
      "train loss:0.08643963589208119\n",
      "train loss:0.13424840446105446\n",
      "train loss:0.048922499935579455\n",
      "train loss:0.035970569700467386\n",
      "train loss:0.047270674661025226\n",
      "train loss:0.0824724216235889\n",
      "train loss:0.06714432714964326\n",
      "train loss:0.05810846104797384\n",
      "train loss:0.041897576027523524\n",
      "train loss:0.07886025135281559\n",
      "train loss:0.11691897385706923\n",
      "train loss:0.049701781058466475\n",
      "train loss:0.10553768322118948\n",
      "train loss:0.07589522747480655\n",
      "train loss:0.03752391801437607\n",
      "train loss:0.05089573664028015\n",
      "train loss:0.21426926622743117\n",
      "train loss:0.14288724460106464\n",
      "train loss:0.0628653470438583\n",
      "train loss:0.05071082145463828\n",
      "train loss:0.11409595860617597\n",
      "train loss:0.03817993088891979\n",
      "train loss:0.08198093327526519\n",
      "train loss:0.08606640848850168\n",
      "train loss:0.08202453946483432\n",
      "train loss:0.10677754598694278\n",
      "train loss:0.12544744924985332\n",
      "train loss:0.08699676316460155\n",
      "train loss:0.12438341568845694\n",
      "train loss:0.04964845003822263\n",
      "train loss:0.0678878100647221\n",
      "train loss:0.10860625518723685\n",
      "train loss:0.07806420355010472\n",
      "train loss:0.12284281661498406\n",
      "train loss:0.08649935050792931\n",
      "train loss:0.05212661857174275\n",
      "train loss:0.034037074422339234\n",
      "train loss:0.04060620798646224\n",
      "train loss:0.09318224081076443\n",
      "train loss:0.04951356403231642\n",
      "train loss:0.10843566328444823\n",
      "train loss:0.12592520653831385\n",
      "train loss:0.07569427181242216\n",
      "train loss:0.127718556283378\n",
      "train loss:0.06930667496326656\n",
      "train loss:0.03148391460066268\n",
      "train loss:0.12862130121515386\n",
      "train loss:0.1079547506155521\n",
      "train loss:0.05536682134856569\n",
      "train loss:0.08071225380769702\n",
      "train loss:0.04362235749059618\n",
      "train loss:0.08979460905233337\n",
      "train loss:0.07477934959918123\n",
      "train loss:0.06161747112862308\n",
      "train loss:0.10797847516277347\n",
      "train loss:0.05530881348036827\n",
      "train loss:0.10338603861497629\n",
      "train loss:0.04117694522704847\n",
      "train loss:0.07141066225342046\n",
      "train loss:0.08448775561821258\n",
      "train loss:0.04898423459142012\n",
      "train loss:0.07253477597714517\n",
      "train loss:0.0646116323456726\n",
      "train loss:0.09035737731084645\n",
      "train loss:0.07717824443854539\n",
      "train loss:0.12010813605050663\n",
      "train loss:0.06374009531988654\n",
      "train loss:0.12144498276563682\n",
      "train loss:0.04779231256747651\n",
      "train loss:0.07390232647247053\n",
      "train loss:0.031632336017223\n",
      "train loss:0.06147734203654289\n",
      "train loss:0.06073950069248242\n",
      "train loss:0.056892264369574105\n",
      "train loss:0.12762636412434353\n",
      "train loss:0.054229988207736765\n",
      "train loss:0.08833723651084988\n",
      "train loss:0.070659246692107\n",
      "train loss:0.04905994348754158\n",
      "train loss:0.040556372980287324\n",
      "train loss:0.06644735033566013\n",
      "train loss:0.17448142058962868\n",
      "train loss:0.01719294214119025\n",
      "train loss:0.08121190020432277\n",
      "train loss:0.1386072086265992\n",
      "train loss:0.07072360405104837\n",
      "train loss:0.12910453451039397\n",
      "train loss:0.038033928008578126\n",
      "train loss:0.045444619531334614\n",
      "train loss:0.0777773570400921\n",
      "train loss:0.07957372152966158\n",
      "train loss:0.07776518837703143\n",
      "train loss:0.03294288239764977\n",
      "train loss:0.060569783929100646\n",
      "train loss:0.1498025840272522\n",
      "train loss:0.04899630317511007\n",
      "train loss:0.07328036826188239\n",
      "train loss:0.1221445925676744\n",
      "train loss:0.032580843042661255\n",
      "train loss:0.1394121182840107\n",
      "train loss:0.10492310590313231\n",
      "train loss:0.040961120978093624\n",
      "train loss:0.10112479574731761\n",
      "train loss:0.08855241083738959\n",
      "train loss:0.04743535481368675\n",
      "train loss:0.05675579121891425\n",
      "train loss:0.04589044981180186\n",
      "train loss:0.020627582370919502\n",
      "train loss:0.10513808114857032\n",
      "train loss:0.08997344241834422\n",
      "train loss:0.040095785213088896\n",
      "train loss:0.07052362391003161\n",
      "train loss:0.06847922424875558\n",
      "train loss:0.08728804145666288\n",
      "train loss:0.03680010457862375\n",
      "train loss:0.0679813608346597\n",
      "train loss:0.05779651797284815\n",
      "train loss:0.06735448030270132\n",
      "train loss:0.12243290312432648\n",
      "train loss:0.07374202117696589\n",
      "train loss:0.10191584391004391\n",
      "train loss:0.05005959821602704\n",
      "train loss:0.11645507171544192\n",
      "train loss:0.08912520948101214\n",
      "train loss:0.12725288415415925\n",
      "train loss:0.038309633200067174\n",
      "train loss:0.155173184004557\n",
      "train loss:0.04955791924650094\n",
      "train loss:0.07372926708019466\n",
      "train loss:0.05900488385689326\n",
      "train loss:0.08985033318467046\n",
      "train loss:0.036803687980054664\n",
      "train loss:0.09384616391650295\n",
      "train loss:0.12478520763203642\n",
      "train loss:0.07898161308994672\n",
      "train loss:0.022477139020904723\n",
      "train loss:0.043148020662131026\n",
      "train loss:0.02999403964342783\n",
      "train loss:0.05118154081415269\n",
      "train loss:0.07673657977596514\n",
      "train loss:0.025128999848750153\n",
      "train loss:0.04655861640284937\n",
      "train loss:0.026694874101262357\n",
      "train loss:0.021825744376100048\n",
      "train loss:0.03431267068095194\n",
      "train loss:0.06519210576136938\n",
      "train loss:0.060403770051383684\n",
      "train loss:0.05456712565305403\n",
      "train loss:0.09553972742124886\n",
      "train loss:0.09074381009903341\n",
      "train loss:0.09425021012629822\n",
      "train loss:0.1086656642049856\n",
      "train loss:0.08270421977215234\n",
      "train loss:0.16859658353171242\n",
      "train loss:0.1461667001922382\n",
      "train loss:0.11769287105543338\n",
      "train loss:0.05637411690427192\n",
      "train loss:0.05329583662330497\n",
      "train loss:0.04155158490148438\n",
      "train loss:0.07914134641565104\n",
      "train loss:0.07087670658978801\n",
      "train loss:0.041484842529995215\n",
      "train loss:0.06783933074678881\n",
      "train loss:0.08365876568866856\n",
      "train loss:0.02998604549289865\n",
      "train loss:0.10260460723343162\n",
      "train loss:0.10165238734725443\n",
      "train loss:0.20055729550016438\n",
      "train loss:0.047778061235351395\n",
      "train loss:0.07061929473864419\n",
      "train loss:0.06396624223173826\n",
      "train loss:0.09902623987469142\n",
      "train loss:0.050522532026679084\n",
      "train loss:0.07315488748622893\n",
      "train loss:0.04579141988474955\n",
      "train loss:0.03832332355481028\n",
      "train loss:0.08286071245536458\n",
      "train loss:0.08115136719902565\n",
      "train loss:0.036426816011915365\n",
      "train loss:0.046377992361638504\n",
      "train loss:0.08937390859701523\n",
      "train loss:0.14633323741646306\n",
      "train loss:0.050021081938928916\n",
      "train loss:0.06577246909992258\n",
      "train loss:0.08122977830167613\n",
      "train loss:0.02187113072068308\n",
      "train loss:0.11036970946046541\n",
      "train loss:0.12258578710807572\n",
      "train loss:0.03497989580094843\n",
      "train loss:0.055433655698049984\n",
      "train loss:0.0546910700978337\n",
      "train loss:0.07265618059951183\n",
      "train loss:0.021733261295132512\n",
      "train loss:0.0371148316634771\n",
      "train loss:0.14495975779274362\n",
      "train loss:0.09017280644327352\n",
      "train loss:0.051532519258419354\n",
      "train loss:0.07285801614039729\n",
      "train loss:0.01845367141325122\n",
      "train loss:0.06580842512510633\n",
      "train loss:0.10479057232097917\n",
      "train loss:0.0480855034665693\n",
      "train loss:0.03717294363743798\n",
      "train loss:0.07403220908681588\n",
      "train loss:0.04233644644849474\n",
      "train loss:0.10097837029799987\n",
      "train loss:0.11730429255664858\n",
      "train loss:0.23548786893073065\n",
      "train loss:0.09140870124678817\n",
      "train loss:0.02390254003861983\n",
      "train loss:0.04251605872570002\n",
      "train loss:0.06938302307128613\n",
      "train loss:0.08180543205814109\n",
      "train loss:0.12927402806960298\n",
      "train loss:0.061477292784870956\n",
      "train loss:0.16177866676643082\n",
      "train loss:0.031138777819645712\n",
      "train loss:0.1871657494064385\n",
      "train loss:0.0955627642566359\n",
      "train loss:0.09088163541940761\n",
      "train loss:0.17056779829322874\n",
      "train loss:0.06379765869645461\n",
      "train loss:0.08818902416651148\n",
      "train loss:0.060387157632708216\n",
      "train loss:0.026524780700416285\n",
      "train loss:0.10769430823024008\n",
      "train loss:0.20136996724698336\n",
      "train loss:0.05808726054493461\n",
      "train loss:0.05828484987940849\n",
      "train loss:0.029199953040785008\n",
      "train loss:0.038916358510573915\n",
      "train loss:0.10557965158654692\n",
      "train loss:0.059015787522904574\n",
      "train loss:0.09557838390353599\n",
      "train loss:0.032684281779309064\n",
      "train loss:0.01121691093311466\n",
      "train loss:0.06920425678220907\n",
      "train loss:0.058229813126785705\n",
      "train loss:0.07250875043909859\n",
      "train loss:0.026295435179072145\n",
      "train loss:0.049677307052356824\n",
      "train loss:0.0769508021430871\n",
      "train loss:0.11783705700318146\n",
      "train loss:0.10644863548243043\n",
      "train loss:0.0749658927365699\n",
      "train loss:0.03337658497513682\n",
      "train loss:0.11791261493501924\n",
      "train loss:0.051572134261843254\n",
      "train loss:0.05014795384895907\n",
      "train loss:0.06179050308950316\n",
      "train loss:0.03480753767214586\n",
      "train loss:0.08181219161620953\n",
      "train loss:0.08146004025808268\n",
      "train loss:0.05287106389197764\n",
      "train loss:0.09459773722762563\n",
      "train loss:0.023448675759129248\n",
      "train loss:0.12011198668432596\n",
      "train loss:0.0397462002458197\n",
      "train loss:0.02686883692806009\n",
      "train loss:0.04928682908629555\n",
      "train loss:0.04043130803885877\n",
      "train loss:0.07297133671342754\n",
      "train loss:0.10158694234754712\n",
      "train loss:0.026088858738442477\n",
      "train loss:0.10108629970508731\n",
      "train loss:0.050331275589671334\n",
      "train loss:0.04634207005404507\n",
      "train loss:0.0480431234820022\n",
      "train loss:0.06239463933514473\n",
      "train loss:0.048463141937756846\n",
      "train loss:0.019825104516122287\n",
      "train loss:0.07509709105317079\n",
      "train loss:0.08106931760865348\n",
      "train loss:0.07976376006743366\n",
      "train loss:0.10472857071784063\n",
      "train loss:0.02098281177042703\n",
      "train loss:0.06996319302196467\n",
      "train loss:0.054723570318814835\n",
      "train loss:0.07936287385762099\n",
      "train loss:0.03196021133218665\n",
      "train loss:0.05905660826791994\n",
      "train loss:0.07894338336547087\n",
      "train loss:0.13357661696582898\n",
      "train loss:0.043513016626198234\n",
      "train loss:0.04069980282370533\n",
      "train loss:0.08568098516482221\n",
      "train loss:0.1284830951802307\n",
      "train loss:0.07098784104798865\n",
      "train loss:0.033773309548951726\n",
      "train loss:0.06510642330821209\n",
      "train loss:0.05669773537233084\n",
      "train loss:0.03355971376437116\n",
      "train loss:0.024382568767719045\n",
      "train loss:0.06070656405098762\n",
      "train loss:0.0328348712540001\n",
      "train loss:0.05608902005218147\n",
      "train loss:0.03676540388057745\n",
      "train loss:0.06487613933455838\n",
      "train loss:0.10770909187741484\n",
      "train loss:0.03536834982488199\n",
      "train loss:0.10847118760587167\n",
      "train loss:0.08933494376031112\n",
      "train loss:0.12392130317677895\n",
      "train loss:0.06092070597951253\n",
      "train loss:0.031036689487840066\n",
      "train loss:0.037274356815784386\n",
      "train loss:0.06585016897231917\n",
      "train loss:0.01641151433570889\n",
      "train loss:0.009033920786126286\n",
      "train loss:0.09768162388924367\n",
      "train loss:0.02801248983625415\n",
      "train loss:0.06984457254595887\n",
      "train loss:0.07571489363410996\n",
      "train loss:0.03558841542355127\n",
      "train loss:0.08387211688981902\n",
      "train loss:0.08547345529868594\n",
      "train loss:0.03514934508636016\n",
      "train loss:0.05473839098822229\n",
      "train loss:0.05763843205017054\n",
      "train loss:0.11203267342515762\n",
      "train loss:0.09375570685469033\n",
      "train loss:0.029150585998383396\n",
      "train loss:0.02583388457456502\n",
      "train loss:0.04232735607672272\n",
      "train loss:0.08079372553451168\n",
      "train loss:0.07717412460960552\n",
      "train loss:0.13714469399763746\n",
      "train loss:0.035427959332918806\n",
      "train loss:0.08436956939073376\n",
      "train loss:0.024312028245777256\n",
      "train loss:0.06778303986794089\n",
      "train loss:0.09194168892311326\n",
      "train loss:0.09478131658578941\n",
      "train loss:0.11389239837203745\n",
      "train loss:0.04968152468126325\n",
      "train loss:0.11811639484275016\n",
      "train loss:0.10701119191819694\n",
      "train loss:0.06009518911970044\n",
      "train loss:0.0788166167414703\n",
      "train loss:0.09822038505456437\n",
      "train loss:0.03609427281879569\n",
      "train loss:0.0653915250264831\n",
      "train loss:0.029051511643992715\n",
      "train loss:0.11609077836005323\n",
      "train loss:0.12687293431762528\n",
      "train loss:0.0958882990113998\n",
      "train loss:0.023073507651271797\n",
      "train loss:0.08176922100780679\n",
      "train loss:0.028402756371769197\n",
      "train loss:0.06843391349011431\n",
      "train loss:0.1288248354637487\n",
      "train loss:0.04896945776614986\n",
      "train loss:0.061047365221702087\n",
      "train loss:0.050016093358054105\n",
      "train loss:0.02808750804444328\n",
      "train loss:0.07088704515337552\n",
      "train loss:0.031902305202636885\n",
      "train loss:0.056550500493752744\n",
      "train loss:0.06929823033354507\n",
      "train loss:0.09023520490949087\n",
      "train loss:0.1290761403735569\n",
      "train loss:0.06038529414467365\n",
      "train loss:0.06930980592151208\n",
      "train loss:0.11300958547627199\n",
      "train loss:0.045920187304398834\n",
      "train loss:0.03339496484418581\n",
      "train loss:0.04311033708590442\n",
      "train loss:0.02541400162827261\n",
      "train loss:0.022030022549915082\n",
      "train loss:0.11170724394596793\n",
      "train loss:0.20571761988825624\n",
      "train loss:0.10887056799376352\n",
      "train loss:0.055433070823212444\n",
      "train loss:0.05813363770651263\n",
      "train loss:0.14348789622455194\n",
      "train loss:0.056859699100933615\n",
      "train loss:0.05447640379716963\n",
      "train loss:0.029810579885602796\n",
      "train loss:0.0538558693288069\n",
      "train loss:0.08566132617018546\n",
      "train loss:0.07159916711677804\n",
      "train loss:0.03482029215194222\n",
      "train loss:0.08425905094977923\n",
      "train loss:0.06281645036599733\n",
      "train loss:0.09379723218876583\n",
      "train loss:0.08696876960033618\n",
      "train loss:0.05094812797033979\n",
      "train loss:0.015262915895061947\n",
      "train loss:0.07661535790850199\n",
      "train loss:0.034882030035428876\n",
      "train loss:0.03385267131081686\n",
      "train loss:0.07964768431072815\n",
      "train loss:0.059874748260438836\n",
      "train loss:0.05680623389044283\n",
      "train loss:0.10129973556167733\n",
      "train loss:0.04445823545016183\n",
      "train loss:0.06531309141080661\n",
      "train loss:0.06931005480966583\n",
      "train loss:0.08847239924176199\n",
      "train loss:0.045715895457239426\n",
      "train loss:0.03476267350988941\n",
      "train loss:0.032824073308098015\n",
      "train loss:0.05451981149455046\n",
      "train loss:0.0745098418116328\n",
      "train loss:0.09585941717046279\n",
      "train loss:0.02020485693214894\n",
      "train loss:0.037098627017470104\n",
      "train loss:0.04761778487250695\n",
      "train loss:0.02490116852763729\n",
      "train loss:0.06719465487232318\n",
      "train loss:0.03714428966823262\n",
      "train loss:0.11457090918026033\n",
      "train loss:0.0552544746586488\n",
      "train loss:0.11167208786265183\n",
      "train loss:0.026413450191831528\n",
      "train loss:0.042794347413666715\n",
      "train loss:0.06551504830361699\n",
      "train loss:0.1289238674132383\n",
      "train loss:0.008064031188534406\n",
      "train loss:0.08443618262999238\n",
      "train loss:0.07711327023794748\n",
      "train loss:0.07570304894138846\n",
      "train loss:0.06489187601482829\n",
      "train loss:0.04805658066196532\n",
      "train loss:0.06551590393969553\n",
      "train loss:0.05840054261875174\n",
      "train loss:0.04030023927168048\n",
      "train loss:0.02254387550768687\n",
      "train loss:0.03379311076716018\n",
      "train loss:0.03252015075582961\n",
      "train loss:0.02158803616846454\n",
      "train loss:0.04218782365491056\n",
      "train loss:0.08647315044859939\n",
      "train loss:0.013325386905091528\n",
      "train loss:0.019525593815207617\n",
      "train loss:0.08041343542603506\n",
      "train loss:0.03561350455404177\n",
      "train loss:0.03260488449438267\n",
      "train loss:0.08386870611684198\n",
      "train loss:0.09112372248226557\n",
      "train loss:0.0244581030770998\n",
      "train loss:0.12311316462961164\n",
      "train loss:0.03941052754478187\n",
      "train loss:0.018185940464145324\n",
      "train loss:0.038193602479786565\n",
      "train loss:0.04425256102024513\n",
      "train loss:0.04160767932121901\n",
      "train loss:0.04224578250350848\n",
      "train loss:0.03208258008979025\n",
      "train loss:0.07965169081217176\n",
      "train loss:0.08637626452858341\n",
      "train loss:0.08263730783869097\n",
      "train loss:0.021754616909077304\n",
      "train loss:0.013720530914180446\n",
      "train loss:0.07945883470083571\n",
      "train loss:0.050106433575082494\n",
      "train loss:0.059989679823468055\n",
      "train loss:0.11252960040315774\n",
      "train loss:0.02029264488485901\n",
      "train loss:0.016201982666874008\n",
      "train loss:0.04680646801575175\n",
      "train loss:0.10528782123229527\n",
      "=== epoch:3, train acc:0.98, test acc:0.979 ===\n",
      "train loss:0.05462382313162199\n",
      "train loss:0.022413285112396188\n",
      "train loss:0.09897263737462629\n",
      "train loss:0.047996997395831825\n",
      "train loss:0.0459443549572511\n",
      "train loss:0.03385723293965269\n",
      "train loss:0.09527555163513841\n",
      "train loss:0.07005296897184325\n",
      "train loss:0.03443856903435456\n",
      "train loss:0.05361600720476604\n",
      "train loss:0.02387251345641472\n",
      "train loss:0.13348489235719888\n",
      "train loss:0.07169872986848033\n",
      "train loss:0.02695059376058782\n",
      "train loss:0.03851973437261277\n",
      "train loss:0.0225176736633475\n",
      "train loss:0.00867538479350271\n",
      "train loss:0.11040600996999507\n",
      "train loss:0.017492472421327118\n",
      "train loss:0.06928908990638873\n",
      "train loss:0.04250612863976826\n",
      "train loss:0.01417494486269282\n",
      "train loss:0.0240391760562981\n",
      "train loss:0.055444375839142045\n",
      "train loss:0.09710046867842374\n",
      "train loss:0.03431447423266325\n",
      "train loss:0.02983551829174231\n",
      "train loss:0.03863124650846841\n",
      "train loss:0.059276077937358555\n",
      "train loss:0.10663814376611101\n",
      "train loss:0.03910231546192209\n",
      "train loss:0.05205183247795559\n",
      "train loss:0.04384554688609064\n",
      "train loss:0.1275255387480016\n",
      "train loss:0.1313416983007289\n",
      "train loss:0.030536118698692064\n",
      "train loss:0.01262765221606677\n",
      "train loss:0.030970973633218116\n",
      "train loss:0.0752005795401241\n",
      "train loss:0.0717900645110014\n",
      "train loss:0.061882134892773985\n",
      "train loss:0.09686040985352923\n",
      "train loss:0.03827909874935417\n",
      "train loss:0.022479226029194336\n",
      "train loss:0.08135456626890797\n",
      "train loss:0.012843201779492023\n",
      "train loss:0.06146505236587999\n",
      "train loss:0.03974531129796559\n",
      "train loss:0.04330278972045339\n",
      "train loss:0.11876346757134636\n",
      "train loss:0.056315248294173344\n",
      "train loss:0.050899906724915685\n",
      "train loss:0.044903996299440835\n",
      "train loss:0.035115509334527506\n",
      "train loss:0.025742762596182786\n",
      "train loss:0.041758406476195366\n",
      "train loss:0.0938979629411876\n",
      "train loss:0.08944221554905525\n",
      "train loss:0.09651638620640156\n",
      "train loss:0.054204840137494756\n",
      "train loss:0.1350735151356093\n",
      "train loss:0.059675957473079\n",
      "train loss:0.05556608956771732\n",
      "train loss:0.0965877001355127\n",
      "train loss:0.050876875266167555\n",
      "train loss:0.006526583750350555\n",
      "train loss:0.06105639414757238\n",
      "train loss:0.01782542476981257\n",
      "train loss:0.05204404746428379\n",
      "train loss:0.07216719634436024\n",
      "train loss:0.04101429779855966\n",
      "train loss:0.032712209841984204\n",
      "train loss:0.023497662833038856\n",
      "train loss:0.03399177981697406\n",
      "train loss:0.07353852462330955\n",
      "train loss:0.06921000572465778\n",
      "train loss:0.031258328651220114\n",
      "train loss:0.049719069149372956\n",
      "train loss:0.04124291118070942\n",
      "train loss:0.1505689570927785\n",
      "train loss:0.03096386852631821\n",
      "train loss:0.06336041321706856\n",
      "train loss:0.06622194443064412\n",
      "train loss:0.023567729006161494\n",
      "train loss:0.07711279064088689\n",
      "train loss:0.023452307091027445\n",
      "train loss:0.0072611882607093\n",
      "train loss:0.019474228121381568\n",
      "train loss:0.09189070251788811\n",
      "train loss:0.10395020427653291\n",
      "train loss:0.056024525531503847\n",
      "train loss:0.06047871986991162\n",
      "train loss:0.033073830099903924\n",
      "train loss:0.055693805509834514\n",
      "train loss:0.1242845304497434\n",
      "train loss:0.07304847246410304\n",
      "train loss:0.02405124226623553\n",
      "train loss:0.06466556579937065\n",
      "train loss:0.08269654793563422\n",
      "train loss:0.05903173337898953\n",
      "train loss:0.014779729051671806\n",
      "train loss:0.05889528537098605\n",
      "train loss:0.03059507620267644\n",
      "train loss:0.025837410747578157\n",
      "train loss:0.08743003818895836\n",
      "train loss:0.08145421855048729\n",
      "train loss:0.03573004624252112\n",
      "train loss:0.008346885710380388\n",
      "train loss:0.03254243821818336\n",
      "train loss:0.06106565693592851\n",
      "train loss:0.030723170076643637\n",
      "train loss:0.019124801634133142\n",
      "train loss:0.10653361169349006\n",
      "train loss:0.07309264122442406\n",
      "train loss:0.047945372403444944\n",
      "train loss:0.027193078394596535\n",
      "train loss:0.049069989005916526\n",
      "train loss:0.04197631503175299\n",
      "train loss:0.038201763526820656\n",
      "train loss:0.02732399677378843\n",
      "train loss:0.04753976572854231\n",
      "train loss:0.05069016647273803\n",
      "train loss:0.048186799689465556\n",
      "train loss:0.074808348568589\n",
      "train loss:0.02268761446093686\n",
      "train loss:0.036467555588667114\n",
      "train loss:0.028118372434552005\n",
      "train loss:0.04164086994469647\n",
      "train loss:0.04552037801845555\n",
      "train loss:0.06976584981740883\n",
      "train loss:0.032988680888315074\n",
      "train loss:0.09895373603286849\n",
      "train loss:0.029967791081120635\n",
      "train loss:0.02719050169328693\n",
      "train loss:0.032289348845128935\n",
      "train loss:0.10301500311491547\n",
      "train loss:0.038562967151665205\n",
      "train loss:0.062183393473936484\n",
      "train loss:0.021540540690771163\n",
      "train loss:0.08121809372363803\n",
      "train loss:0.0606761912288307\n",
      "train loss:0.029536162665636162\n",
      "train loss:0.10247333954189675\n",
      "train loss:0.03980200453338945\n",
      "train loss:0.04780778013197314\n",
      "train loss:0.10385913669059431\n",
      "train loss:0.03575561961851552\n",
      "train loss:0.07157795275954783\n",
      "train loss:0.01676035116993563\n",
      "train loss:0.021753415226466028\n",
      "train loss:0.07081603101927075\n",
      "train loss:0.03282301674775614\n",
      "train loss:0.03073593597394147\n",
      "train loss:0.023386832189677555\n",
      "train loss:0.0821436286366353\n",
      "train loss:0.08645759852986562\n",
      "train loss:0.011242412206119972\n",
      "train loss:0.05590242962363694\n",
      "train loss:0.042009256139756236\n",
      "train loss:0.12593387371341117\n",
      "train loss:0.07775840068084858\n",
      "train loss:0.17163156356477655\n",
      "train loss:0.04955389391251547\n",
      "train loss:0.06970155381299174\n",
      "train loss:0.03360315336300798\n",
      "train loss:0.08633785755730829\n",
      "train loss:0.06584446801564409\n",
      "train loss:0.0395114811235736\n",
      "train loss:0.020511860171239644\n",
      "train loss:0.040312619200736276\n",
      "train loss:0.046526871732327765\n",
      "train loss:0.033759272951861694\n",
      "train loss:0.07393815415839368\n",
      "train loss:0.02898346295450907\n",
      "train loss:0.03806276619707415\n",
      "train loss:0.06735928694896026\n",
      "train loss:0.06094576261543984\n",
      "train loss:0.050398313213334746\n",
      "train loss:0.06290465417316066\n",
      "train loss:0.033398651083618704\n",
      "train loss:0.04243742918187143\n",
      "train loss:0.03436943696491952\n",
      "train loss:0.08982740483774176\n",
      "train loss:0.03282647063540971\n",
      "train loss:0.02319928971848391\n",
      "train loss:0.05349859744982112\n",
      "train loss:0.08802496012949719\n",
      "train loss:0.04267238650729952\n",
      "train loss:0.06424849223438793\n",
      "train loss:0.032378832703587664\n",
      "train loss:0.023012673947155317\n",
      "train loss:0.02379638738311114\n",
      "train loss:0.023021594706158114\n",
      "train loss:0.012753645792990957\n",
      "train loss:0.053872994210341404\n",
      "train loss:0.04228647936396533\n",
      "train loss:0.01947533510994718\n",
      "train loss:0.046108123002048106\n",
      "train loss:0.08763097867113402\n",
      "train loss:0.11677666169796279\n",
      "train loss:0.017015624324840394\n",
      "train loss:0.07238385400759856\n",
      "train loss:0.04574056893583389\n",
      "train loss:0.045504606264937485\n",
      "train loss:0.0773960120077672\n",
      "train loss:0.008632045359343678\n",
      "train loss:0.12855307959130083\n",
      "train loss:0.057277887891133104\n",
      "train loss:0.08897504286718362\n",
      "train loss:0.10665427900158099\n",
      "train loss:0.07591973107383693\n",
      "train loss:0.0174353707252963\n",
      "train loss:0.23691215965396112\n",
      "train loss:0.10374432060975641\n",
      "train loss:0.08628644514539768\n",
      "train loss:0.034741187380109\n",
      "train loss:0.06006449314965959\n",
      "train loss:0.05624622592381783\n",
      "train loss:0.16805133866899344\n",
      "train loss:0.02207477071708121\n",
      "train loss:0.015573609361958222\n",
      "train loss:0.10765914637625724\n",
      "train loss:0.08872530858824605\n",
      "train loss:0.028473576512036847\n",
      "train loss:0.05257085416859705\n",
      "train loss:0.10371361641442058\n",
      "train loss:0.01934684308601824\n",
      "train loss:0.026203147998240067\n",
      "train loss:0.029896613675581992\n",
      "train loss:0.04634688487583731\n",
      "train loss:0.09892530971730201\n",
      "train loss:0.07310407289666128\n",
      "train loss:0.05104361018587369\n",
      "train loss:0.07179068882310088\n",
      "train loss:0.025894160061527716\n",
      "train loss:0.05749966574061757\n",
      "train loss:0.029327822893178587\n",
      "train loss:0.027782872547559383\n",
      "train loss:0.03992020232709932\n",
      "train loss:0.017033863836397326\n",
      "train loss:0.052745256593282\n",
      "train loss:0.07624554976835832\n",
      "train loss:0.013928455070186814\n",
      "train loss:0.07780678532384046\n",
      "train loss:0.0235886828941952\n",
      "train loss:0.09463227588954057\n",
      "train loss:0.014747264617757903\n",
      "train loss:0.07223243507151995\n",
      "train loss:0.06711515242264234\n",
      "train loss:0.019438256087788996\n",
      "train loss:0.032903439379232645\n",
      "train loss:0.06761873411081462\n",
      "train loss:0.023582333932135095\n",
      "train loss:0.06366464201208259\n",
      "train loss:0.030785618114208727\n",
      "train loss:0.034864260159720765\n",
      "train loss:0.11451978972743085\n",
      "train loss:0.15313065096880313\n",
      "train loss:0.09521818833235095\n",
      "train loss:0.20599926573484367\n",
      "train loss:0.08410866277735994\n",
      "train loss:0.02947020412613908\n",
      "train loss:0.037163437930957735\n",
      "train loss:0.13392524978522555\n",
      "train loss:0.04789033589516027\n",
      "train loss:0.07572935169410015\n",
      "train loss:0.048973418255019024\n",
      "train loss:0.05652394218021357\n",
      "train loss:0.020388235737745525\n",
      "train loss:0.00993464315672871\n",
      "train loss:0.1414073984082144\n",
      "train loss:0.11258529256328567\n",
      "train loss:0.05246198667565385\n",
      "train loss:0.050320703343470316\n",
      "train loss:0.02121066638923511\n",
      "train loss:0.05520513005196233\n",
      "train loss:0.11714111458205892\n",
      "train loss:0.06466449528242137\n",
      "train loss:0.11920441083978504\n",
      "train loss:0.03899206554122042\n",
      "train loss:0.03427773603772241\n",
      "train loss:0.06436109220789388\n",
      "train loss:0.03475306118107276\n",
      "train loss:0.040088933033683036\n",
      "train loss:0.04043491895325425\n",
      "train loss:0.04390679387087292\n",
      "train loss:0.05255731977645664\n",
      "train loss:0.03558501654915176\n",
      "train loss:0.07984931202480477\n",
      "train loss:0.09519310653224076\n",
      "train loss:0.07495866663464394\n",
      "train loss:0.03222439954536123\n",
      "train loss:0.08226561737233755\n",
      "train loss:0.0319918244265416\n",
      "train loss:0.04165944094412132\n",
      "train loss:0.021523315269071922\n",
      "train loss:0.0433351533227878\n",
      "train loss:0.02621950561523037\n",
      "train loss:0.022044113904067544\n",
      "train loss:0.05651542688276855\n",
      "train loss:0.03568578787022664\n",
      "train loss:0.07207288393021893\n",
      "train loss:0.10657447901315882\n",
      "train loss:0.15399965414240296\n",
      "train loss:0.07425514283092666\n",
      "train loss:0.06819188485804356\n",
      "train loss:0.13416764504516218\n",
      "train loss:0.1244315142833576\n",
      "train loss:0.04591889018489039\n",
      "train loss:0.02661569610623558\n",
      "train loss:0.04122353608708493\n",
      "train loss:0.15764887409095862\n",
      "train loss:0.03015386668043683\n",
      "train loss:0.03927211203456439\n",
      "train loss:0.027764840997875607\n",
      "train loss:0.0868067089715491\n",
      "train loss:0.025382430801495385\n",
      "train loss:0.040689822218976913\n",
      "train loss:0.025403453597312043\n",
      "train loss:0.02106385164289842\n",
      "train loss:0.07792375987632334\n",
      "train loss:0.03748636903279498\n",
      "train loss:0.030077018318586553\n",
      "train loss:0.04029076630319508\n",
      "train loss:0.027585673980402677\n",
      "train loss:0.03592323788939233\n",
      "train loss:0.1348505539828904\n",
      "train loss:0.014449401975453921\n",
      "train loss:0.00707981465489324\n",
      "train loss:0.03470963316850686\n",
      "train loss:0.052828753514220325\n",
      "train loss:0.03708647081194056\n",
      "train loss:0.011583225109553428\n",
      "train loss:0.02276314274399394\n",
      "train loss:0.03132759546192193\n",
      "train loss:0.018770369959350632\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\chapter7.ipynb 셀 15\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m network \u001b[39m=\u001b[39m SimpleConvNet(input_dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m), \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                         conv_param \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfilter_num\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m30\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfilter_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                         hidden_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, weight_init_std\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                   epochs\u001b[39m=\u001b[39mmax_epochs, mini_batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                   optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, optimizer_param\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.001\u001b[39m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                   evaluate_sample_num_per_epoch\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# 매개변수 보존\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%EB%8C%80%ED%95%99%EA%B5%90/%EB%8F%99%EC%95%84%EB%A6%AC/%EC%BD%94%EC%8A%A4%EB%AF%B9/2022%20%EC%97%AC%EB%A6%84%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%8A%A4%ED%84%B0%EB%94%94/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EA%B9%83%ED%97%88%EB%B8%8C/deep-learning-from-cosmic/mc/chapter7/chapter7.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m network\u001b[39m.\u001b[39msave_params(\u001b[39m\"\u001b[39m\u001b[39mparams.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\trainer.py:70\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     69\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m---> 70\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[0;32m     72\u001b[0m     test_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39maccuracy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_test)\n\u001b[0;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\trainer.py:46\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparams, grads)\n\u001b[1;32m---> 46\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mloss(x_batch, t_batch)\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain loss:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss))\n",
      "File \u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\simple_convnet.py:75\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m     68\u001b[0m     \u001b[39m\"\"\"손실 함수를 구한다.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m    t : 정답 레이블\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_layer\u001b[39m.\u001b[39mforward(y, t)\n",
      "File \u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\simple_convnet.py:63\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     62\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m---> 63\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\layers.py:261\u001b[0m, in \u001b[0;36mPooling.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    258\u001b[0m out_h \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m (H \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_h) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride)\n\u001b[0;32m    259\u001b[0m out_w \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m (W \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_w) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride)\n\u001b[1;32m--> 261\u001b[0m col \u001b[39m=\u001b[39m im2col(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool_h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool_w, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad)\n\u001b[0;32m    262\u001b[0m col \u001b[39m=\u001b[39m col\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_h\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_w)\n\u001b[0;32m    264\u001b[0m arg_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\대학교\\동아리\\코스믹\\2022 여름 인공지능 스터디\\인공지능 깃허브\\deep-learning-from-cosmic\\mc\\chapter7\\util.py:58\u001b[0m, in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     55\u001b[0m out_h \u001b[39m=\u001b[39m (H \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpad \u001b[39m-\u001b[39m filter_h)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mstride \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     56\u001b[0m out_w \u001b[39m=\u001b[39m (W \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpad \u001b[39m-\u001b[39m filter_w)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mstride \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 58\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mpad(input_data, [(\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m), (\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m), (pad, pad), (pad, pad)], \u001b[39m'\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m col \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((N, C, filter_h, filter_w, out_h, out_w))\n\u001b[0;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(filter_h):\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bmc05\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraypad.py:793\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m stat_functions \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmaximum\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mamax, \u001b[39m\"\u001b[39m\u001b[39mminimum\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mamin,\n\u001b[0;32m    789\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmean, \u001b[39m\"\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmedian}\n\u001b[0;32m    791\u001b[0m \u001b[39m# Create array with final shape and original values\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[39m# (padded area is undefined)\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m padded, original_area_slice \u001b[39m=\u001b[39m _pad_simple(array, pad_width)\n\u001b[0;32m    794\u001b[0m \u001b[39m# And prepare iteration over all dimensions\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[39m# (zipping may be more readable than using enumerate)\u001b[39;00m\n\u001b[0;32m    796\u001b[0m axes \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(padded\u001b[39m.\u001b[39mndim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.6 CNN 시각화하기  \n",
    "CNN을 구성하는 합성곱 계층은 입력으로 받은 이미지 데이터에서 '무엇을 보고 있는' 것일까?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1번째 층의 합성곱 계층의 가중치는 그 형상이 (30, 1, 5, 5,)였다. 필터의 크기가 5x5이고 채널이 1개라는 것은 이 필터를 1채널의 회색조 이미지로 시각화할 수 있다는 뜻이다.  \n",
    "학습 전 필터는 무작위로 초기화 되어있어 흑백의 정도에 규칙이 없다.  \n",
    "학습을 마친 필터는 규칙성 있는 이미지가 되었다. -> 에지(색상이 바뀐 경계선), 블롭(국소적으로 덩어리진 영역)이 보인다.  \n",
    "\n",
    "1번째 층의 합성곱 계층에서는 에지나 블롭 등의 저수준 정보가 추출된다.  \n",
    "계층이 깊어질수록 추출되는 정보(정확히는 강하게 반응하는 뉴런)는 더 추상화된다.\n",
    "\n",
    "합성곱 계층을 여러 겹 쌓으면 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다.  \n",
    "즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 '고급' 정보로 변화해간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.7 대표적인 CNN\n",
    "LeNet: 손글씨 숫자를 인식하는 네트워크로, 1998년에 제안되었다.  \n",
    "합성곱 계층과 풀링 계층('원소를 줄이기'만 하는 서브샘플링 계층)을 반복하고 마지막으로 완전연결 계층을 거치면서 결과를 출력한다.  \n",
    "\n",
    "LeNet과 현재의 CNN의 차이점\n",
    "LeNet은 활성화 함수로 시그모이드 함수를 사용하는데 현재의 CNN은 ReLU를 사용  \n",
    "LeNet은 서브샘플링을 하여 중간 데이터의 크기가 작아지지만 현재의 CNN은 최대 풀링이 주류다.(데이터 크기 안줄어든다.)  \n",
    "\n",
    "AlexNet은 2012년에 발표된 딥러닝 네트워크로, 딥러닝 열풍을 일으키는 데 큰 역할을 했다.  \n",
    "LeNet에서 큰 구조는 바뀌지 않았지만, AlexNet에서 다음과 같은 변화가 있다.  \n",
    "* 활성화 함수로 ReLU 사용  \n",
    "* LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층 이용  \n",
    "* 드롭아웃 사용  \n",
    "\n",
    "대량의 데이터를 누구나 얻을 수 있게 되었고, 병렬 계산에 특화된 GPU가 보급되면서 대량의 연산을 고속으로 수행할 수 있게 됨\n",
    "-> 빅데이터, GPU가 딥러닝 발전의 큰 원동력  \n",
    "\n",
    "7.8 정리  \n",
    "* CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.  \n",
    "* 합성곱 계층과 풀링 계층은 im2col(이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.  \n",
    "* CNN을 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다.  \n",
    "* 대표적인 CNN에는 LeNet과 AlexNet이 있다.  \n",
    "* 딥러닝의 발전에는 빅데이터와 GPU가 크게 기여했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88f5a4554b5a9ef949b5de2ca977b5a4bd361180c5b30f32147c76c2c5bcf452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
